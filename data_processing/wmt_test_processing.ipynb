{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "870678ff-c2a8-4ff1-b003-9b7fe8125f0d",
   "metadata": {},
   "source": [
    "## WMT Test Set Processing\n",
    "This notebook will be used for devising a script to parse WMT BTT test sets.\n",
    "\n",
    "### Start State\n",
    "WMT Test sets from 2020, 2021 and 2022 each comprise four files - test sentences, gold sentences, document mapping to ID, and sentence alignment.\n",
    "\n",
    "### Desired Outcome\n",
    "We have two different outcomes. These outcomes depend on the format most convenient for the utilities we will use.\n",
    "- For our **test set**, which is the WMT 2022 BTT test set, we want to generate two .txt files of aligned sentences. The first .txt file contains source sentences s<sub>1</sub>, s<sub>2</sub> ... s<sub>N</sub>, while the second .txt file contains target sentences t<sub>1</sub>, t<sub>2</sub> ... t<sub>N</sub>, where t<sub>i</sub> is the gold-standard translation of s<sub>i</sub>. This format is most convenient for the Transformers `pipeline` utility.\n",
    "- For our **validation set**, which comprises the concatenated 2020 and 2021 test sets, we want to generate a single .txt file containing aligned pairs of source and target sentences in the format `source_sentence (TAB) target_sentence (newline)`. This is more convenient for evaluation sets, for which we will use the HuggingFace `Trainer` API.\n",
    "\n",
    "### Considerations\n",
    "- HuggingFace's programmatic Dataset upload API is broken; this issue is still unresolved and cannot be overcome by switching to earlier versions. We therefore have to preprocess and upload via the web interface.\n",
    "- We cannot use a CSV, because the `datasets` library's to_csv function adds strange characters to the data, even with the correct UTF encoding applied. In essence, converting to CSV and uploading _that_ leads to data corruption. Intuitively, we must use a .txt file and upload that via the web interface - end-users will load our dataset as a Dataset or DatasetDict object, and the .txt file source will be transparent to them. I'll write up a script to process these .txt files, since they will all be in the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f43af910-2370-44f4-a3e9-b926b1656b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get current working directory\n",
    "import os, pandas as pd\n",
    "dir_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3ded1c1-19a1-439c-9340-073cb6970554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Organise file path names first\n",
    "YEARS = [20, 21, 22]\n",
    "FILE_NAMES = [\"alignment.tsv\", \"gold.txt\", \"mapping.txt\", \"test.txt\"]\n",
    "FILE_PATHS = { YEARS[j] : [\"../data/wmt\" + str(YEARS[j]) + \"_enfr_test/wmt\" + str(YEARS[j]) + \"_enfr_\" + FILE_NAMES[i] for i in range(len(FILE_NAMES))] for j in range(len(YEARS))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c608a21-f461-48ca-831f-34bdc7d60c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start with 2022\n",
    "files_2022 = FILE_PATHS[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e0b8407-bed8-4fc3-8091-609f1d5346f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect and read content into a list of pandas dataframes\n",
    "content = {\"alignment\":\"\", \"gold\":\"\", \"mapping\":\"\", \"test\":\"\"} #Helps us identify which is which\n",
    "numMap = {0 : \"alignment\", 1: \"gold\", 2 : \"mapping\", 3 : \"test\"}\n",
    "col_names = {0 : [\"alignmentQuality\", \"ID\", \"targetSentenceNum\", \"sourceSentenceNum\"], 1 : [\"docNum\", \"targetSentenceNum\", \"sentence\"], 2 : [\"ID\", \"docNum\"], 3 : [\"docNum\", \"sourceSentenceNum\", \"sentence\"]}\n",
    "for i in range(len(files_2022)):\n",
    "    f = open(os.path.join(dir_path, files_2022[i]))\n",
    "    #print(repr(f.readline())) #All our files are tab-separated! This allows us to use pandas as follows:\n",
    "    content[numMap[i]] = pd.read_csv(f, delimiter = \"\\t\", header=None, names = col_names[i])\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f15ba77-113c-4be8-8a23-2c988ec12452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test our ability to write output without data corruption - seems okay from what I can see\n",
    "\"\"\"\n",
    "f = open(\"testoutput.txt\", \"w\")\n",
    "for i in range(len(content[\"gold\"])):\n",
    "    f.write(content[\"gold\"][\"sentence\"][i])\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f15f985-aa4f-4d6d-9468-e0435fc28d99",
   "metadata": {},
   "source": [
    "Our first step is to modify the alignments by substituting document numbers for IDs. We'll add an extra column for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36853e79-906e-41cb-bb4f-996b2f054b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert our mappings dataframe into a dictionary\n",
    "mappings = dict(content[\"mapping\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18873b74-1e46-4d8b-a6f7-a002644da381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alignmentQuality</th>\n",
       "      <th>ID</th>\n",
       "      <th>targetSentenceNum</th>\n",
       "      <th>sourceSentenceNum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OK</td>\n",
       "      <td>35096693</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK</td>\n",
       "      <td>35096693</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OK</td>\n",
       "      <td>35096693</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OK</td>\n",
       "      <td>35096693</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OK</td>\n",
       "      <td>35096693</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>NO_ALIGNMENT</td>\n",
       "      <td>19144122</td>\n",
       "      <td>omitted</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>NO_ALIGNMENT</td>\n",
       "      <td>19144122</td>\n",
       "      <td>omitted</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>NO_ALIGNMENT</td>\n",
       "      <td>19144122</td>\n",
       "      <td>omitted</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>NO_ALIGNMENT</td>\n",
       "      <td>19144122</td>\n",
       "      <td>omitted</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>NO_ALIGNMENT</td>\n",
       "      <td>19144122</td>\n",
       "      <td>omitted</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>813 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    alignmentQuality        ID targetSentenceNum sourceSentenceNum\n",
       "0                 OK  35096693                 1                 1\n",
       "1                 OK  35096693                 2                 2\n",
       "2                 OK  35096693                 3                 3\n",
       "3                 OK  35096693                 4                 4\n",
       "4                 OK  35096693                 5                 5\n",
       "..               ...       ...               ...               ...\n",
       "808     NO_ALIGNMENT  19144122           omitted                26\n",
       "809     NO_ALIGNMENT  19144122           omitted                27\n",
       "810     NO_ALIGNMENT  19144122           omitted                28\n",
       "811     NO_ALIGNMENT  19144122           omitted                29\n",
       "812     NO_ALIGNMENT  19144122           omitted                30\n",
       "\n",
       "[813 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[\"alignment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab3a387c-5b34-47c0-ae61-8f5f2af3dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new column with corresponding document numbers\n",
    "alignments = content[\"alignment\"]\n",
    "alignments[\"docNum\"] = alignments[\"ID\"].map(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d770f007-b058-49cd-888a-7209f525dbab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alignmentQuality</th>\n",
       "      <th>ID</th>\n",
       "      <th>targetSentenceNum</th>\n",
       "      <th>sourceSentenceNum</th>\n",
       "      <th>docNum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OK</td>\n",
       "      <td>35096693</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>doc19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK</td>\n",
       "      <td>35096693</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>doc19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OK</td>\n",
       "      <td>35096693</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>doc19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OK</td>\n",
       "      <td>35096693</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>doc19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OK</td>\n",
       "      <td>35096693</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>doc19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>NO_ALIGNMENT</td>\n",
       "      <td>19144122</td>\n",
       "      <td>omitted</td>\n",
       "      <td>26</td>\n",
       "      <td>doc11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>NO_ALIGNMENT</td>\n",
       "      <td>19144122</td>\n",
       "      <td>omitted</td>\n",
       "      <td>27</td>\n",
       "      <td>doc11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>NO_ALIGNMENT</td>\n",
       "      <td>19144122</td>\n",
       "      <td>omitted</td>\n",
       "      <td>28</td>\n",
       "      <td>doc11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>NO_ALIGNMENT</td>\n",
       "      <td>19144122</td>\n",
       "      <td>omitted</td>\n",
       "      <td>29</td>\n",
       "      <td>doc11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>NO_ALIGNMENT</td>\n",
       "      <td>19144122</td>\n",
       "      <td>omitted</td>\n",
       "      <td>30</td>\n",
       "      <td>doc11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>813 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    alignmentQuality        ID targetSentenceNum sourceSentenceNum docNum\n",
       "0                 OK  35096693                 1                 1  doc19\n",
       "1                 OK  35096693                 2                 2  doc19\n",
       "2                 OK  35096693                 3                 3  doc19\n",
       "3                 OK  35096693                 4                 4  doc19\n",
       "4                 OK  35096693                 5                 5  doc19\n",
       "..               ...       ...               ...               ...    ...\n",
       "808     NO_ALIGNMENT  19144122           omitted                26  doc11\n",
       "809     NO_ALIGNMENT  19144122           omitted                27  doc11\n",
       "810     NO_ALIGNMENT  19144122           omitted                28  doc11\n",
       "811     NO_ALIGNMENT  19144122           omitted                29  doc11\n",
       "812     NO_ALIGNMENT  19144122           omitted                30  doc11\n",
       "\n",
       "[813 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b2e2ccc-3c07-4150-9aee-6e54f2e59c8f",
   "metadata": {},
   "source": [
    "Then, we must filter out only the OK sentence alignments and query the gold and test dataframes with these. Weirdly enough, some OK alignments omitted the target and source sentence numbers, perhaps due to truncated abstracts. So far, so good - this is still WMT BTT protocol, because they only used the OK aligned test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecdfcc10-6752-4827-af0a-e6a9c1eb77f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_alignments = alignments.loc[(alignments[\"alignmentQuality\"] == \"OK\") & \n",
    "(alignments[\"targetSentenceNum\"].str.contains(\"omitted\") == False) & \n",
    "(alignments[\"sourceSentenceNum\"].str.contains(\"omitted\") == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4903dfad-cbbc-4ce2-b35e-61b544ddcfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '14,15' '11,12' '10,11'\n",
      " '27' '9,10' '4,5' '2,3']\n",
      "['1' '2' '3' '4' '5' '6' '7' '8,9' '10' '11' '12' '13' '14' '15' '16' '8'\n",
      " '9' '17' '18' '1,2' '19' '20' '21,22' '23' '24' '25' '26' '27' '28' '21'\n",
      " '22' '4,5']\n"
     ]
    }
   ],
   "source": [
    "print(ok_alignments.targetSentenceNum.unique())\n",
    "print(ok_alignments.sourceSentenceNum.unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3258a007-5204-48a7-b507-2a14265057d2",
   "metadata": {},
   "source": [
    "We also notice that some sentences align to multiple sentences, perhaps due to semicolons. This shouldn't be a big problem - just concatenate the comma-separated sentences together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaa5a6a2-321c-4f8d-8bd0-6eebefd32000",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_alignments = ok_alignments.astype({\"sourceSentenceNum\": str, \"targetSentenceNum\": str, \"docNum\" : str})\n",
    "gold = content[\"gold\"].astype({\"targetSentenceNum\": str, \"docNum\" : str, \"sentence\" : str})\n",
    "test = content[\"test\"].astype({\"sourceSentenceNum\": str, \"docNum\" : str, \"sentence\" : str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61e8851d-8732-4881-8a52-a9d873616950",
   "metadata": {},
   "outputs": [],
   "source": [
    "testSentences = ok_alignments[[\"sourceSentenceNum\", \"docNum\"]]\n",
    "f = open(\"wmt22test.txt\", \"w\")\n",
    "for index, row in testSentences.iterrows():\n",
    "    queries = row[\"sourceSentenceNum\"].split(\",\")\n",
    "    buffer = \"\"\n",
    "    for query in queries:\n",
    "        if query != queries[-1]:\n",
    "            buffer += test.loc[(test[\"sourceSentenceNum\"] == query) & (test[\"docNum\"] == row[\"docNum\"])][\"sentence\"].values[0] + \" \"\n",
    "        else:\n",
    "            buffer += test.loc[(test[\"sourceSentenceNum\"] == query) & (test[\"docNum\"] == row[\"docNum\"])][\"sentence\"].values[0]\n",
    "    f.write(buffer + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0119145-62d8-4d50-a07c-44fbb56c3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "goldSentences = ok_alignments[[\"targetSentenceNum\", \"docNum\"]]\n",
    "f = open(\"wmt22gold.txt\", \"w\")\n",
    "for index, row in goldSentences.iterrows():\n",
    "    queries = row[\"targetSentenceNum\"].split(\",\")\n",
    "    buffer = \"\"\n",
    "    for query in queries:\n",
    "        if query != queries[-1]:\n",
    "            buffer += gold.loc[(gold[\"targetSentenceNum\"] == query) & (gold[\"docNum\"] == row[\"docNum\"])][\"sentence\"].values[0] + \" \"\n",
    "        else:\n",
    "            buffer += gold.loc[(gold[\"targetSentenceNum\"] == query) & (gold[\"docNum\"] == row[\"docNum\"])][\"sentence\"].values[0]\n",
    "    f.write(buffer + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60924bc4-abb5-49ce-83b5-a3ea723e9d36",
   "metadata": {},
   "source": [
    "So far, so good. A quick inspection reveals that there are some misalignments (e.g., doc22), but because this matches up with the provided alignment file, we won't modify it - it might artificially inflate our scores relative to the WMT SOTA. Let's move on to the WMT 2021 and WMT 2020 datasets, beginning with 2021. We can do the exact same thing, and then read from both text files. We write into another text file, filling in with \\t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "337d6826-1d00-48e6-8535-7ef35c3dcda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_2021 = FILE_PATHS[21]\n",
    "files_2020 = FILE_PATHS[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "532bdb92-69bd-46ed-941e-c6b5d38249f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(files_2021)):\n",
    "    f = open(os.path.join(dir_path, files_2021[i]), encoding = \"utf8\")\n",
    "    #print(repr(f.readline())) #All our files are still tab-separated! This allows us to use pandas as follows:\n",
    "    content[numMap[i]] = pd.read_csv(f, delimiter = \"\\t\", header=None, encoding = \"utf8\", names = col_names[i])\n",
    "    f.close()\n",
    "mappings = dict(content[\"mapping\"].values)\n",
    "alignments = content[\"alignment\"]\n",
    "alignments[\"docNum\"] = alignments[\"ID\"].map(mappings)\n",
    "ok_alignments = alignments.loc[(alignments[\"alignmentQuality\"] == \"OK\") & \n",
    "(alignments[\"targetSentenceNum\"].str.contains(\"omitted\") == False) & \n",
    "(alignments[\"sourceSentenceNum\"].str.contains(\"omitted\") == False)]\n",
    "ok_alignments\n",
    "ok_alignments = ok_alignments.astype({\"sourceSentenceNum\": str, \"targetSentenceNum\": str, \"docNum\" : str})\n",
    "gold = content[\"gold\"].astype({\"targetSentenceNum\": str, \"docNum\" : str, \"sentence\" : str})\n",
    "test = content[\"test\"].astype({\"sourceSentenceNum\": str, \"docNum\" : str, \"sentence\" : str})\n",
    "testSentences = ok_alignments[[\"sourceSentenceNum\", \"docNum\"]]\n",
    "#At this point we had an error - the organisers omitted quite a few documents entirely from both gold and test sets.\n",
    "#print(testSentences.docNum.unique() == gold.docNum.unique())\n",
    "#print(gold.docNum.unique() == test.docNum.unique()) #Evidently, both gold and test cohere.\n",
    "testSentences = testSentences.loc[testSentences[\"docNum\"].isin(gold.docNum.unique())]\n",
    "f = open(\"wmt21test.txt\", \"w\", encoding = \"utf8\")\n",
    "for index, row in testSentences.iterrows():\n",
    "    queries = row[\"sourceSentenceNum\"].split(\",\")\n",
    "    buffer = \"\"\n",
    "    for query in queries:\n",
    "        if query != queries[-1]:\n",
    "            buffer += test.loc[(test[\"sourceSentenceNum\"] == query) & (test[\"docNum\"] == row[\"docNum\"])][\"sentence\"].values[0] + \" \"\n",
    "        else:\n",
    "            buffer += test.loc[(test[\"sourceSentenceNum\"] == query) & (test[\"docNum\"] == row[\"docNum\"])][\"sentence\"].values[0]\n",
    "    f.write(buffer + \"\\n\")\n",
    "f.close()\n",
    "goldSentences = ok_alignments[[\"targetSentenceNum\", \"docNum\"]]\n",
    "goldSentences = goldSentences.loc[goldSentences[\"docNum\"].isin(gold.docNum.unique())]\n",
    "f = open(\"wmt21gold.txt\", \"w\", encoding = \"utf8\")\n",
    "for index, row in goldSentences.iterrows():\n",
    "    queries = row[\"targetSentenceNum\"].split(\",\")\n",
    "    buffer = \"\"\n",
    "    for query in queries:\n",
    "        if query != queries[-1]:\n",
    "            buffer += gold.loc[(gold[\"targetSentenceNum\"] == query) & (gold[\"docNum\"] == row[\"docNum\"])][\"sentence\"].values[0] + \" \"\n",
    "        else:\n",
    "            buffer += gold.loc[(gold[\"targetSentenceNum\"] == query) & (gold[\"docNum\"] == row[\"docNum\"])][\"sentence\"].values[0]\n",
    "    f.write(buffer + \"\\n\")\n",
    "f.close()\n",
    "with open(\"wmt21test.txt\", encoding = \"utf8\") as f1, open(\"wmt21gold.txt\", encoding = \"utf8\") as f2:\n",
    "    test_list = [line.rstrip('\\n') for line in f1]\n",
    "    gold_list = [line.rstrip('\\n') for line in f2]\n",
    "f1.close()\n",
    "f2.close()\n",
    "output = open(\"validation.txt\", \"w\", encoding = \"utf8\")\n",
    "for i in range(len(test_list)):\n",
    "    output.write(test_list[i] + \"\\t\" + gold_list[i] + \"\\n\")\n",
    "output.close() #Inspecting the text file, it looks alright."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e656ff6a-09cd-4690-9199-dcca158fcc40",
   "metadata": {},
   "source": [
    "Having settled 2021, let's append 2020's content to validation.txt. We encountered issues because the source set missed out sentence 2 of doc80 and sentence 6 of doc54 entirely. Because we aren't sure which maps to which, let's remove those rows from the alignment mapping. We also encountered issues because the csv reader was unable to parse sentence 9 of doc96 due to double quotation marks in sentence 8 - we'll remove the row containing source sentence 8, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ce3da88-c336-4322-89ef-eb115ee11809",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(files_2020)):\n",
    "    f = open(os.path.join(dir_path, files_2020[i]), encoding = \"utf8\")\n",
    "    #print(repr(f.readline())) #All our files are still tab-separated! This allows us to use pandas as follows:\n",
    "    content[numMap[i]] = pd.read_csv(f, delimiter = \"\\t\", header=None, encoding = \"utf8\", names = col_names[i])\n",
    "    f.close()\n",
    "mappings = dict(content[\"mapping\"].values)\n",
    "alignments = content[\"alignment\"]\n",
    "alignments[\"docNum\"] = alignments[\"ID\"].map(mappings)\n",
    "ok_alignments = alignments.loc[(alignments[\"alignmentQuality\"] == \"OK\") & \n",
    "(alignments[\"targetSentenceNum\"].str.contains(\"omitted\") == False) & \n",
    "(alignments[\"sourceSentenceNum\"].str.contains(\"omitted\") == False)]\n",
    "ok_alignments\n",
    "ok_alignments = ok_alignments.astype({\"sourceSentenceNum\": str, \"targetSentenceNum\": str, \"docNum\" : str})\n",
    "gold = content[\"gold\"].astype({\"targetSentenceNum\": str, \"docNum\" : str, \"sentence\" : str})\n",
    "test = content[\"test\"].astype({\"sourceSentenceNum\": str, \"docNum\" : str, \"sentence\" : str})\n",
    "\n",
    "#Hacky way to drop the offending columns, because abstracts aren't that long\n",
    "to_drop = ((\"2\", \"doc80\"), (\"6\", \"doc54\"), (\"8\", \"doc96\"))\n",
    "for item in to_drop:\n",
    "    dropped_index = (ok_alignments.loc[(ok_alignments[\"sourceSentenceNum\"].str.contains(\"1\" + item[0]) == False) & (ok_alignments[\"sourceSentenceNum\"].str.contains(item[0])) & \n",
    "                     (ok_alignments.docNum == item[1])]).index\n",
    "    ok_alignments = ok_alignments.drop(dropped_index)\n",
    "testSentences = ok_alignments[[\"sourceSentenceNum\", \"docNum\"]]\n",
    "\n",
    "#At this point we had the same error again - the organisers omitted quite a few documents entirely from both gold and test sets.\n",
    "#print(gold.docNum.unique() == test.docNum.unique()) #Evidently, both gold and test cohere.\n",
    "testSentences = testSentences.loc[testSentences[\"docNum\"].isin(gold.docNum.unique())]\n",
    "f = open(\"wmt20test.txt\", \"w\", encoding = \"utf8\")\n",
    "for index, row in testSentences.iterrows():\n",
    "    queries = row[\"sourceSentenceNum\"].split(\",\")\n",
    "    buffer = \"\"\n",
    "    for query in queries:\n",
    "        try:\n",
    "            if query != queries[-1]:\n",
    "                buffer += test.loc[(test[\"sourceSentenceNum\"] == query) & (test[\"docNum\"] == row[\"docNum\"])][\"sentence\"].values[0] + \" \"\n",
    "            else:\n",
    "                buffer += test.loc[(test[\"sourceSentenceNum\"] == query) & (test[\"docNum\"] == row[\"docNum\"])][\"sentence\"].values[0]\n",
    "        except:\n",
    "            print(\"Issue with source query: \" + query + \" and document: \" + row[\"docNum\"])\n",
    "    f.write(buffer + \"\\n\")\n",
    "f.close()\n",
    "goldSentences = ok_alignments[[\"targetSentenceNum\", \"docNum\"]]\n",
    "goldSentences = goldSentences.loc[goldSentences[\"docNum\"].isin(gold.docNum.unique())]\n",
    "f = open(\"wmt20gold.txt\", \"w\", encoding = \"utf8\")\n",
    "for index, row in goldSentences.iterrows():\n",
    "    queries = row[\"targetSentenceNum\"].split(\",\")\n",
    "    buffer = \"\"\n",
    "    for query in queries:\n",
    "        try:\n",
    "            if query != queries[-1]:\n",
    "                buffer += gold.loc[(gold[\"targetSentenceNum\"] == query) & (gold[\"docNum\"] == row[\"docNum\"])][\"sentence\"].values[0] + \" \"\n",
    "            else:\n",
    "                buffer += gold.loc[(gold[\"targetSentenceNum\"] == query) & (gold[\"docNum\"] == row[\"docNum\"])][\"sentence\"].values[0]\n",
    "        except:\n",
    "            print(\"Issue with target query: \" + query + \" and document: \" + row[\"docNum\"])\n",
    "    f.write(buffer + \"\\n\")\n",
    "f.close()\n",
    "with open(\"wmt20test.txt\", encoding = \"utf8\") as f1, open(\"wmt20gold.txt\", encoding = \"utf8\") as f2:\n",
    "    test_list = [line.rstrip('\\n') for line in f1]\n",
    "    gold_list = [line.rstrip('\\n') for line in f2]\n",
    "f1.close()\n",
    "f2.close()\n",
    "output = open(\"validation.txt\", \"a\", encoding = \"utf8\")\n",
    "for i in range(len(test_list)):\n",
    "    output.write(test_list[i] + \"\\t\" + gold_list[i] + \"\\n\")\n",
    "output.close() #Inspecting the text file, it looks alright."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
