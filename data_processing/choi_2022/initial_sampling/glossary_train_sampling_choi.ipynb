{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf1f554e-12dc-4225-ad2f-1a337193d8ef",
   "metadata": {},
   "source": [
    "# Data Processing for Choi et al. (2022)'s Soft Constraint Method\n",
    "\n",
    "We first filter the clean glossary - which already only contains one-to-one translations (saving us one preprocessing step) - to remove terms occurring at high frequency within a general-domain corpus. We use our Lexique data for this. After that, we will sample 15% of our training data and save the other 85% to Huggingface. We will then use Spacy and SimAlign to tokenise and word-align the sampled data, but that will be for the HPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09e00691-34f1-4d52-b198-395d671476c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "#Converts data in src [TAB] tgt [NEWLINE] format to a format suitable for model training\n",
    "def convertToDictFormat(data):\n",
    "    source = []\n",
    "    target = []\n",
    "    for example in data:\n",
    "        example = example.strip()\n",
    "        sentences = example.split(\"\\t\")\n",
    "        source.append(sentences[0])\n",
    "        target.append(sentences[1])\n",
    "    ready = Dataset.from_dict({\"en\":source, \"fr\":target})\n",
    "    return ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65fc775c-4670-4b9a-82d2-3becd9b4cebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (C:/Users/ethan/.cache/huggingface/datasets/ethansimrm___text/ethansimrm--MeSpEn_enfr_cleaned_glossary-e1fb9a6b67efd03c/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    }
   ],
   "source": [
    "#Load in clean glossary and convert to Dataset object\n",
    "term_candidates = load_dataset(\"ethansimrm/MeSpEn_enfr_cleaned_glossary\", split = \"train\")\n",
    "terms_ready = convertToDictFormat(term_candidates['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78a2fb61-2b27-4e3b-a0a0-7d03721201aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_fr = pd.read_excel(\"Lexique_FR_PoS.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "027330d6-c37a-4be2-8b97-374aa3edda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the frequency of each word in Lexique (this is by no means perfect; Choi et al. had a massive OOD corpus to check against). \n",
    "def word_frequency(row):\n",
    "    query = row[\"fr\"].lower() #All our Lexique words are in lowercase\n",
    "    ans = most_common_fr[most_common_fr[\"Word\"] == query] #Search for word or lemma match\n",
    "    if (ans.empty):\n",
    "        ans = most_common_fr[most_common_fr[\"lemme\"] == query]\n",
    "        if (ans.empty):\n",
    "            return row\n",
    "    if (len(ans) == 1):\n",
    "        row[\"general_freq\"] = ans[\"freqfilms2\"].values[0]\n",
    "    else:\n",
    "        row[\"general_freq\"] = ans[\"freqfilms2\"].max() \n",
    "        #If we have more than one match, assume the most frequent form (due to plurals, etc.) - we are looking for the prevalence of this concept in the general domain\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "691219fd-8712-4350-8819-b95fe3333bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5084 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "general_freq = [-1] * len(terms_ready) #Create a new placeholder column\n",
    "terms_ready_new = terms_ready.add_column(\"general_freq\", general_freq)\n",
    "terms_ready_with_freq = terms_ready_new.map(word_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51c79d68-d29e-49c8-905d-128e31e5b01e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 'breast', 'fr': 'sein', 'general_freq': 27.97}\n",
      "{'en': 'Bruise', 'fr': 'Bleu', 'general_freq': 21.63}\n",
      "{'en': 'buttock', 'fr': 'fesse', 'general_freq': 34.52}\n",
      "{'en': 'child', 'fr': 'enfant', 'general_freq': 448.33}\n",
      "{'en': 'color', 'fr': 'couleur', 'general_freq': 24.79}\n",
      "{'en': 'consideration', 'fr': 'fait', 'general_freq': 27.36}\n",
      "{'en': 'data', 'fr': 'données', 'general_freq': 20.05}\n",
      "{'en': 'death', 'fr': 'mort', 'general_freq': 78.09}\n",
      "{'en': 'drugs', 'fr': 'médicaments', 'general_freq': 29.8}\n",
      "{'en': 'ear', 'fr': 'oreille', 'general_freq': 39.08}\n",
      "{'en': 'Error', 'fr': 'Erreur', 'general_freq': 22.82}\n",
      "{'en': 'Family', 'fr': 'Famille', 'general_freq': 27.16}\n",
      "{'en': 'fasting', 'fr': 'jeune', 'general_freq': 62.12}\n",
      "{'en': 'film', 'fr': 'film', 'general_freq': 57.56}\n",
      "{'en': 'finger', 'fr': 'doigt', 'general_freq': 45.86}\n",
      "{'en': 'Finger', 'fr': 'Doigt', 'general_freq': 45.86}\n",
      "{'en': 'hair', 'fr': 'cheveu', 'general_freq': 116.16}\n",
      "{'en': 'Hazard', 'fr': 'Risque', 'general_freq': 23.36}\n",
      "{'en': 'hub', 'fr': 'garde', 'general_freq': 27.7}\n",
      "{'en': 'Information', 'fr': 'Information', 'general_freq': 39.3}\n",
      "{'en': 'insult', 'fr': 'cris', 'general_freq': 26.79}\n",
      "{'en': 'knee', 'fr': 'genou', 'general_freq': 42.82}\n",
      "{'en': 'knees', 'fr': 'genoux', 'general_freq': 42.82}\n",
      "{'en': 'limb', 'fr': 'membre', 'general_freq': 29.34}\n",
      "{'en': 'medications', 'fr': 'médicaments', 'general_freq': 29.8}\n",
      "{'en': 'medicine', 'fr': 'médicaments', 'general_freq': 29.8}\n",
      "{'en': 'patient', 'fr': 'patient', 'general_freq': 22.49}\n",
      "{'en': 'Physician', 'fr': 'Médecin', 'general_freq': 27.84}\n",
      "{'en': 'portal', 'fr': 'porte', 'general_freq': 45.46}\n",
      "{'en': 'Potency', 'fr': 'Force', 'general_freq': 43.67}\n",
      "{'en': 'ratio', 'fr': 'rapport', 'general_freq': 24.51}\n",
      "{'en': 'Relationship', 'fr': 'Relation', 'general_freq': 30.92}\n",
      "{'en': 'Risk', 'fr': 'Risque', 'general_freq': 23.36}\n",
      "{'en': 'running', 'fr': 'course', 'general_freq': 32.04}\n",
      "{'en': 'site', 'fr': 'endroit', 'general_freq': 21.48}\n",
      "{'en': 'straight', 'fr': 'droit', 'general_freq': 36.33}\n",
      "{'en': 'strength', 'fr': 'force', 'general_freq': 43.67}\n",
      "{'en': 'stroke', 'fr': 'coup', 'general_freq': 82.33}\n",
      "{'en': 'teeth', 'fr': 'dents', 'general_freq': 60.94}\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "#There are 39 terms which occur with frequency > 20 per 1,000,000 words of our French film subtititles corpus. Choi et al. did not specify what \"high frequency\" meant, so I use \n",
    "#10 per 1,000,000 here as a first pass. All terms here seem relatively general in nature; going down to 10 per 1,000,000 incorporates translations of more medical terms like \"bunion\", \n",
    "#which is \"oignon\" (also the translation of \"onion\") in French. The implicit assumption that general translation is equivalent to the medical translation may not hold in that case - \n",
    "#the baseline model might only emit an \"oignon\" if it is provided with \"onion\", rather than \"bunion\".\n",
    "total = 0\n",
    "for term in terms_ready_with_freq:\n",
    "    if (term[\"general_freq\"] > 20):\n",
    "        total+=1\n",
    "        print(term)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "02793685-9966-45d2-b449-ee9ec349adb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5084 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "terms_ready_filtered = terms_ready_with_freq.filter(lambda x: x[\"general_freq\"] <= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2c127272-f8df-4b53-b73a-2da4afe86d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5045"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(terms_ready_filtered) #Initially, we had 5084 terms; we now have 5045."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "805b0320-afeb-4088-bd18-8a77fe234a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"filtered_cleaned_glossary_choi.txt\", \"w\", encoding = \"utf8\")\n",
    "for term in terms_ready_filtered:\n",
    "    f.write(term['en'] + '\\t' + term['fr'] + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b91e95d8-b7fb-4c0f-baaf-140bdeb4e985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (C:/Users/ethan/.cache/huggingface/datasets/ethansimrm___text/ethansimrm--wmt_16_19_22_biomed_train_processed-8662b34233d7661e/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    }
   ],
   "source": [
    "#Now, load in our training data for splitting\n",
    "train_data = load_dataset(\"ethansimrm/wmt_16_19_22_biomed_train_processed\", split = \"train\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "569074ff-280d-427f-b4e4-d8dd698ba0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\ethan\\.cache\\huggingface\\datasets\\ethansimrm___text\\ethansimrm--wmt_16_19_22_biomed_train_processed-8662b34233d7661e\\0.0.0\\cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\\cache-5f4731a6d62e6ea7.arrow and C:\\Users\\ethan\\.cache\\huggingface\\datasets\\ethansimrm___text\\ethansimrm--wmt_16_19_22_biomed_train_processed-8662b34233d7661e\\0.0.0\\cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\\cache-cf343edd631ba6c0.arrow\n"
     ]
    }
   ],
   "source": [
    "#Seed = 42 for reproducibility. Now, we will store these splits separately in HF. Happily, they are already in our SRC [TAB] TGT [NEWLINE] format.\n",
    "train_data_split = train_data.train_test_split(train_size = 0.15, seed = 42)\n",
    "data_for_choi = train_data_split[\"train\"]\n",
    "unchanged_data = train_data_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ffaa501c-eeff-45b8-8dec-fc7474f89cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"train_data_to_modify_choi.txt\", \"w\", encoding = \"utf8\")\n",
    "for line in data_for_choi['text']:\n",
    "    f.write(line + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "af222134-1447-4698-82e5-8814c629bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"train_data_leave_unchanged_choi.txt\", \"w\", encoding = \"utf8\")\n",
    "for line in unchanged_data['text']:\n",
    "    f.write(line + '\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
