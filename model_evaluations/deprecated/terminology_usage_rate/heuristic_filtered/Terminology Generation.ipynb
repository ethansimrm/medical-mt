{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ae10a3-d2a1-4820-b819-2be0610c4bdb",
   "metadata": {},
   "source": [
    "## Terminology Generation\n",
    "\n",
    "The aim here is to build a list of candidate terminologies from a dataset, when no external dictionaries are available. This allows us to generate an independent terminology baseline for our terminology usage rate (i.e., terminology recall) metric. By doing this, we are independent from dictionaries used in training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "650d7700-f75d-40eb-9365-0d47f7869dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "dir_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05f938b5-eb56-482d-9076-19a06e1251d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open our test files first\n",
    "f = open(os.path.join(dir_path, \"../../wmt22test.txt\"), \"r\", encoding = \"utf8\")\n",
    "en_sent = [line.strip() for line in f.readlines()]\n",
    "f.close()\n",
    "f = open(os.path.join(dir_path, \"../../wmt22gold.txt\"), \"r\", encoding = \"utf8\")\n",
    "fr_sent = [line.strip() for line in f.readlines()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ff742bf-0010-422b-8b7b-0b1a79e89b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in our PoS taggers - we choose transformers for accuracy in tokenisation and PoS-tagging - we are working on the \"gold standard\" after all.\n",
    "#Choi et al. (2022) use Spacy as well, as do Ballier et al. (2022). This is good enough for tokenisation and PoS-tagging - we will manually ID terminology due to issues with NER accuracy.\n",
    "import spacy\n",
    "en_tagger = spacy.load(\"en_core_web_trf\")\n",
    "fr_tagger = spacy.load(\"fr_dep_news_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e48d4d41-1ead-45f5-9d64-adb95f134eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [00:41<00:00, 14.30it/s]\n"
     ]
    }
   ],
   "source": [
    "#We know that medical terminology mostly comprises nouns and adjectives. This is where Part-of-speech tagging comes in!\n",
    "#Removing conjugated verbs decreases noise, too - there are many forms due to masc/fem/plural etc. But first, let's tokenise and PoS-tag our sentences in line with Choi et al. (2022).\n",
    "en_tagged = []\n",
    "for sentence in tqdm(en_sent):\n",
    "    en_tagged_sent = en_tagger(sentence)\n",
    "    en_tagged_tokenised = [token for token in en_tagged_sent] #Access PoS tag information later on\n",
    "    en_tagged.append(en_tagged_tokenised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07b99e9f-a31d-4ad4-9d31-96a601851666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [00:46<00:00, 12.55it/s]\n"
     ]
    }
   ],
   "source": [
    "fr_tagged = []\n",
    "for sentence in tqdm(fr_sent):\n",
    "    fr_tagged_sent = fr_tagger(sentence)\n",
    "    fr_tagged_tokenised = [token for token in fr_tagged_sent]\n",
    "    fr_tagged.append(fr_tagged_tokenised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8ecfbe3-f9d6-480d-8ffa-c28920993618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-07-06 21:43:10,771 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: bert-base-multilingual-cased\n"
     ]
    }
   ],
   "source": [
    "#Now, let's perform word alignments. We couldn't use this to filter data because it does not output probabilities, but we can use this to compute word alignments.\n",
    "#Terminologies must be translated, so we expect good word alignments for them.\n",
    "from simalign import SentenceAligner\n",
    "aligner = SentenceAligner(matching_methods=\"a\") #Argmax only; the simAlign paper stated that this gives the best performance for English-French alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "129e0351-90d4-41ce-a9e9-778ea247c0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [01:25<00:00,  6.88it/s]\n"
     ]
    }
   ],
   "source": [
    "#We will pull out the token text, align that, then check the PoS information.\n",
    "alignments_list = []\n",
    "for i in tqdm(range(len(en_tagged))):\n",
    "    src_sent = [token.text for token in en_tagged[i]]\n",
    "    tgt_sent = [token.text for token in fr_tagged[i]] \n",
    "    alignments = aligner.get_word_aligns(src_sent, tgt_sent)\n",
    "    alignments_list.append(alignments[\"inter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b60412b-122d-47ad-9cf9-370cae981111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string #Need to filter out punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9e1cc6e8-9726-4ff8-ba60-89cee8d00dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want all parts of speech which refer to specific descriptors (ADJ), actions (VERB), and objects (NOUN/PROPN), as these could be specific to the medical domain. Let's save those and inspect them.\n",
    "output = open(\"candidate_terminology.txt\", \"w\", encoding = \"utf8\")\n",
    "for i in range(len(alignments_list)):\n",
    "    for aligned_pair in alignments_list[i]:\n",
    "        src_ind = aligned_pair[0]\n",
    "        tgt_ind = aligned_pair[1]\n",
    "        src_tok = en_tagged[i][src_ind]\n",
    "        tgt_tok = fr_tagged[i][tgt_ind]\n",
    "        if (((src_tok.pos_ == \"NOUN\") and (tgt_tok.pos_ == \"NOUN\")) or \n",
    "        ((src_tok.pos_ == \"ADJ\") and (tgt_tok.pos_ == \"ADJ\")) or\n",
    "        ((src_tok.pos_ == \"VERB\") and (tgt_tok.pos_ == \"VERB\")) or\n",
    "        ((src_tok.pos_ == \"PROPN\") and (tgt_tok.pos_ == \"PROPN\"))):\n",
    "            if not ((src_tok.text in string.punctuation) or (tgt_tok.text in string.punctuation)): #Ignore punctuation; these aren't terminology because they are found everywhere, e.g., %\n",
    "                output.write(str(i) + \"\\t\" + src_tok.text + \"\\t\" + tgt_tok.text + \"\\n\") #We include sentence numbers so we can check terminology for every sentence in the test set.\n",
    "output.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6b470f0-3fda-41f4-8d26-f8c7c36c9f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "candidates = pd.read_csv(\"candidate_terminology.txt\", sep = \"\\t\", header = None, names = [\"sent_ID\", \"en\", \"fr\"])\n",
    "#candidates = candidates.drop_duplicates().reset_index() #Don't drop duplicates - these arise because the same word might appear more than once in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c22846af-b63a-43ce-a365-244fcbb1afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#So far, so good, but we have a few misalignments (unavoidable due to test set misalignments) and some general vocabulary sprinkled in, e.g., man/woman, etc.\n",
    "#We don't need to accord general vocabulary the same importance as medical terminology. This means that we should filter based on general-domain frequency, because terminology is rare.\n",
    "#Let's see how many words are captured using this approach, and what sort of words they are.\n",
    "most_common_fr = pd.read_excel(\"Lexique_FR_PoS.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0a73c0f-e454-4ee5-be70-23c7545c1220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's begin by checking the frequency of each word in Lexique. \n",
    "def word_frequency(row):\n",
    "    query = row[\"fr\"].lower() #All our Lexique words are in lowercase\n",
    "    ans = most_common_fr[most_common_fr[\"Word\"] == query] #Search for word or lemma match\n",
    "    if (ans.empty):\n",
    "        ans = most_common_fr[most_common_fr[\"lemme\"] == query]\n",
    "        if (ans.empty):\n",
    "            return -1\n",
    "    if (len(ans) == 1):\n",
    "        return ans[\"freqfilms2\"].values[0]\n",
    "    else:\n",
    "        return ans[\"freqfilms2\"].max() #If we have more than one match, assume the most frequent form (due to plurals, etc.) - we are looking for the prevalence of this concept in the general domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e73b438-8435-432b-9fc9-4bd1e710138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates['frequency'] = candidates.apply(word_frequency, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd735818-d779-4755-8448-4cccf9ae33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#candidates_sorted = candidates.sort_values(by=['frequency']).reset_index()\n",
    "#Upon inspection, it is very difficult to make a clean cut, because there are some general terms which aren't in our general-domain corpus at all(!), like months = mois. \n",
    "#Conversely, there are some medical terms, like \"medicine\", which are very frequent. We need to compromise - what do we consider terminology?\n",
    "#Park et al. (2002) state that the domain-specificity of a term is given by its probability of occurrence in a domain-specific corpus divided by its occurrence in a general corpus.\n",
    "#Normalising for corpus length, a domain-specific term should occur far more often in the test set compared to a general corpus.\n",
    "#Thankfully, we have tokenised the test set, which also rids us of the articles (l', d', etc.), allowing easier comparison. The downside is that punctuation comes in, so we must account for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c63cbe6f-d48f-4b79-ac1f-766f5c1f9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_the_words = []\n",
    "word_count = 0;\n",
    "for sentence in fr_tagged:\n",
    "    for word in sentence:\n",
    "        if (word.text not in string.punctuation):\n",
    "            all_the_words.append(word.text.lower()) #Avoid capitalisation issues\n",
    "            word_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8ba9c068-812d-4204-bc0e-c08896310b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15476"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count #15476 non-punctuation tokens, i.e., \"words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7a4cb105-4060-455a-bab1-784fb09e3b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11363\n"
     ]
    }
   ],
   "source": [
    "word_count_en = 0;\n",
    "for sentence in en_tagged:\n",
    "    for word in sentence:\n",
    "        if (word.text not in string.punctuation):\n",
    "            word_count_en += 1\n",
    "print(word_count_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16e9c357-fd61-439c-99c1-dc37094db5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "in_domain_counts = Counter(all_the_words) #Frequency of all the words in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8d0d43f-39d9-4750-b4a1-16eacf42b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_domain_freq(row):\n",
    "    query = row[\"fr\"].lower() #All our words are in lowercase\n",
    "    return (in_domain_counts[query] / word_count) * 1000000 #We were given frequency per 1000000 words for the general corpus, so we must upscale to get frequency per \"million words\" here too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16288797-7155-454d-910c-6abee8daa16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates['ID_frequency'] = candidates.apply(in_domain_freq, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "113f31ba-5fc9-455b-a681-f309b9179004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "candidates[\"domain_specificity\"] = np.log(candidates[\"ID_frequency\"] / (candidates[\"frequency\"] + 1.01)) #Account for negative and zero values, and compress the scale because our values are large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "389d9368-574a-4c13-b89b-cbe912cebe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_sorted = candidates.sort_values(by=['domain_specificity']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b249f74b-2146-4629-84b5-c498d0aebc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore data to find the best cutoff prior to manual removal\n",
    "candidates_sorted.to_csv(\"candidates_sorted_by_domain_specificity.tsv\", sep = \"\\t\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cf426937-77a7-4516-8365-e4a5de4ad94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "junk = candidates_sorted[candidates_sorted[\"domain_specificity\"] < 3] #Justifies choice of 3 as a conservative cutoff - most of these words are non-terminological.\n",
    "output = open(\"non_term_list_lexique.txt\", \"w\", encoding = \"utf8\")\n",
    "for i in range(len(junk[\"en\"])):\n",
    "    output.write(str(junk[\"sent_ID\"][i]) + \"\\t\" + junk[\"en\"][i] + \"\\t\" + junk[\"fr\"][i] +  \"\\n\")\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c89f5ba4-59c7-468f-839a-75943b5f7179",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_filtered = candidates_sorted[candidates_sorted[\"domain_specificity\"] >= 3].reset_index(drop=True)\n",
    "#3 is a conservative cutoff - there are a few non-terminology words here, but not that many compared to < 3. This aims to be a superset of a human-annotated terminology anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "02cab15c-2d4f-479d-9996-b66208fd6272",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_filtered_counts = candidates_filtered[[\"sent_ID\", \"en\", \"fr\"]]\n",
    "candidates_filtered_counts = candidates_filtered_counts.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58519e8f-a23b-4204-a90f-88fb3d8b3262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will do exact matching\n",
    "def find_count_in_sentence(row):\n",
    "    query = row[\"fr\"]\n",
    "    return len([found for found in fr_tagged[row[\"sent_ID\"]] if query == found.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cbd6ba7a-82b0-4773-968c-9fa5e56baa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_filtered_counts[\"count\"] = candidates_filtered_counts.apply(find_count_in_sentence, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f9759e30-eafd-4e23-a20f-de8dd5340ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open(\"terminology_heuristic_pre_manual.txt\", \"w\", encoding = \"utf8\")\n",
    "for i in range(len(candidates_filtered_counts[\"fr\"])):\n",
    "    output.write(str(candidates_filtered_counts[\"sent_ID\"][i]) + \"\\t\" + candidates_filtered_counts[\"en\"][i] + \"\\t\" + \n",
    "                 candidates_filtered_counts[\"fr\"][i] + \"\\t\" + str(candidates_filtered_counts[\"count\"][i]) + \"\\n\")\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "174a2d74-9835-451d-ab91-2a1105e42b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_manual = open(\"MANUAL_DONE.txt\", \"r\", encoding = \"utf8\")\n",
    "input_removed = open(\"non_term_list_manual.txt\", \"r\", encoding = \"utf8\")\n",
    "input_manual_lines = [(line.strip() + \"\\n\") for line in input_manual.readlines()]\n",
    "input_removed_lines = [(line.strip() + \"\\n\") for line in input_removed.readlines()]\n",
    "input_manual.close()\n",
    "input_removed.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e20e7d62-cab5-47a6-8d83-412dbd879654",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_manual = open(\"term_list_manual_filtered.txt\", \"w\", encoding = \"utf8\")\n",
    "for line in input_manual_lines:\n",
    "    output_manual.write(line)\n",
    "output_manual.close()\n",
    "output_removed = open(\"removed_term_list_first_pass.txt\", \"w\", encoding = \"utf8\")\n",
    "for line in input_removed_lines:\n",
    "    output_removed.write(line)\n",
    "output_removed.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "26bba407-9242-4fb8-afb2-5d4229042f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Out of 3409 candidate terms, we have eliminated 1914, leaving us with 1495 terms we deem important to their respective sentences. \n",
    "#I'm somewhat confident in the 1498 terms, but we should look at the eliminated terms again, just in case we've missed something important.\n",
    "#We will sort by sentence ID and spit it back out into a .txt file.\n",
    "output_removed = pd.read_csv(\"removed_term_list_first_pass.txt\", sep = \"\\t\", header = None, names = [\"sent_ID\", \"en\", \"fr\", \"count\"])\n",
    "output_removed_sorted = output_removed.sort_values(by = [\"sent_ID\"]).reset_index(drop=True)\n",
    "output_removed_sorted.to_csv(\"removed_term_list_first_pass_sorted.txt\", sep = \"\\t\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f13edc7f-1e56-4822-9ec4-00e75c4a4608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We also do the same for our selected terms, just in case we've made some errors during selection.\n",
    "output_manual = pd.read_csv(\"term_list_manual_filtered.txt\", sep = \"\\t\", header = None, names = [\"sent_ID\", \"en\", \"fr\", \"count\"])\n",
    "output_manual_sorted = output_manual.sort_values(by = [\"sent_ID\"]).reset_index(drop=True)\n",
    "output_manual_sorted.to_csv(\"term_list_manual_filtered_sorted.txt\", sep = \"\\t\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fd9cdcf6-e2d2-4778-aac1-8a052a0fdf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will now go sentence by sentence, looking at both sides and considering importance to sentence meaning. If changing a word greatly influences the meaning of a sentence, that\n",
    "#word is deemed to be terminology. E.g., Gram-POSITIVE vs Gram-NEGATIVE --> both \"positive\" and \"negative\" are important to get right (these weren't picked up by Lexique).\n",
    "#Why didn't we do this at the start? Because it was easier to remove very common non-terminological words (e.g., \"month/mois\") in batches.\n",
    "#The final output is in final_list.txt and final_removed_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c558be54-9103-4907-b759-2fb308806a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, to deduplicate, sort, and create the final list\n",
    "input_final_list = pd.read_csv(\"final_list.txt\", sep = \"\\t\", header = None, names = [\"sent_ID\", \"en\", \"fr\", \"count\"])\n",
    "input_final_list_fr_only = input_final_list[[\"sent_ID\", \"fr\"]]\n",
    "input_final_list_fr_only_dedup = input_final_list_fr_only.drop_duplicates().reset_index(drop=True)\n",
    "input_final_list_fr_only_dedup[\"count\"] = input_final_list_fr_only_dedup.apply(find_count_in_sentence, axis=1)\n",
    "input_final_list_fr_only_dedup = input_final_list_fr_only_dedup.sort_values(by = [\"sent_ID\"]).reset_index(drop=True)\n",
    "input_final_list_fr_only_dedup.to_csv(\"wmt22_gold_terminology_manual.txt\", sep = \"\\t\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7452b071-42f9-4fe8-97e4-c51723c3d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we will generate a separate list without casing, and aggregate counts as usual. Interestingly, we have the same number of rows, indicating that there are no terminologies which \n",
    "#appear multiple times within a single sentence, but with different casing. This means that we can stop here for now.\n",
    "#term_list_uncased = input_final_list_fr_only_dedup[[\"sent_ID\", \"fr\"]]\n",
    "#term_list_uncased[\"uncased_fr\"] = term_list_uncased[\"fr\"].apply(str.lower)\n",
    "#term_list_uncased = term_list_uncased.drop(columns = \"fr\")\n",
    "#term_list_uncased = term_list_uncased.drop_duplicates().reset_index(drop=True)\n",
    "#term_list_uncased"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
