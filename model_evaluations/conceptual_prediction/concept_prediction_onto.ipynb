{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31269289-a4c9-4dec-847c-8f26a2454690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.error, urllib.parse\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "REST_URL = \"https://services.bioportal.lirmm.fr\"\n",
    "API_KEY = \"97be3b10-804a-4c98-9407-05caf1629ebb\"\n",
    "ONTO_SELECTED = \"&ontologies=CISP-2,SNOMED35VF,CIF,WHO-ARTFRE,STY,ATCFRE,CIM-11,MEDLINEPLUS,MTHMSTFRE,MSHFRE,MDRFRE\" #All French UMLS and SNOMED 3.5 ontologies\n",
    "OPTIONS_1 = \"&longest_only=true&exclude_numbers=false&whole_word_only=true&exclude_synonyms=false&expand_mappings=false&fast_context=false&certainty=false\" #Match longest only because we want the whole concept, not the parts\n",
    "OPTIONS_2 = \"&temporality=false&experiencer=false&negation=false&lemmatize=false&score_threshold=0&confidence_threshold=0&display_links=false&display_context=false\"\n",
    "#We wanted to use fast_context, but this ran into several problems parsing punctuation. We'll leave this for future work.\n",
    "PREFERENCE_STRING = ONTO_SELECTED + OPTIONS_1 + OPTIONS_2\n",
    "\n",
    "def get_json(url):\n",
    "    opener = urllib.request.build_opener()\n",
    "    opener.addheaders = [('Authorization', 'apikey token=' + API_KEY)]\n",
    "    return json.loads(opener.open(url).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3865c60b-1bec-47b4-8769-ff53a4a2b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.getcwd()\n",
    "f = open(os.path.join(dir_path, \"../wmt22gold.txt\"), \"r\", encoding = \"utf8\")\n",
    "fr_sent = [line.strip() for line in f.readlines()]\n",
    "f.close()\n",
    "\n",
    "#Get annotations\n",
    "annotations_fr = []\n",
    "for sentence in tqdm(fr_sent): #Use the ICL VPN for this - it seems to get privileged access\n",
    "    #Additionally, the annotator uses \"%20\" to break up words, and therefore encounters problems with sentences which have the % symbol.\n",
    "    #As I judge that it will take too long for me to understand how to locally download and query ontologies (involving SPARQL), I'd rather not waste time reinventing the wheel.\n",
    "    #We will replace \"%\" with \"pour cent\" (FR translation) to preserve meaning whenever it is present. #We add spaces to ensure stuff like \"95%\" is parsed correctly on the receiving end.\n",
    "    sentence = sentence.replace(\"%\", \" pour cent \")\n",
    "    annotations_per_sentence = []\n",
    "    annotations = get_json(REST_URL + \"/annotator?text=\" + urllib.parse.quote(sentence) + PREFERENCE_STRING)\n",
    "    #We must disambiguate duplicate term occurrences from the same term being annotated by different ontologies.\n",
    "    #Because we take the longest annotated sequence, any duplicates will \"hit\" the same subsequence. We can exploit this.\n",
    "    found = set()\n",
    "    for result in annotations:\n",
    "        tag_loc = (result[\"annotations\"][0][\"from\"], result[\"annotations\"][0][\"to\"]) \n",
    "        #Concepts which occur in different ontologies but refer to the same words will have the same from/to on their first annotation\n",
    "        if (tag_loc not in found):\n",
    "            info = (result[\"annotatedClass\"][\"prefLabel\"], len(result[\"annotations\"])) #Take a count of concepts across the sentence\n",
    "            annotations_per_sentence.append(info)\n",
    "            found.add(tag_loc)\n",
    "    annotations_fr.append(annotations_per_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "48446060-83a9-4092-add4-1692c961e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate results\n",
    "sent_IDs = []\n",
    "concepts = []\n",
    "counts = []\n",
    "for i in range(len(annotations_fr)):\n",
    "    for annotation_info in annotations_fr[i]:\n",
    "        sent_IDs.append(i)\n",
    "        concepts.append(annotation_info[0])\n",
    "        counts.append(annotation_info[1])\n",
    "term_list = pd.DataFrame(data = {\"sent_ID\" : sent_IDs, \"concept\" : concepts, \"count\" : counts})\n",
    "term_list.to_csv(\"gold_onto_concepts.txt\", sep = \"\\t\", header = True, index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a95edd20-70c5-4168-86bb-8c43a0c282ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, repeat this workflow for all predictions we've gathered so far - 32 of them\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "base_path = \"../predictions/opus_en_fr_base/\"\n",
    "big_path = \"../predictions/opus_en_fr_big/\"\n",
    "base_files = [join(base_path, f) for f in listdir(base_path) if isfile(join(base_path, f))] #16 files, each comprising all our term predictions\n",
    "big_files = [join(big_path, f) for f in listdir(big_path) if isfile(join(big_path, f))]\n",
    "all_filenames = base_files + big_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5be62b9a-4d82-40cb-9f51-7abe14fac86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [03:26<00:00,  2.85it/s]\n",
      "100%|██████████| 588/588 [03:41<00:00,  2.66it/s]\n",
      "100%|██████████| 588/588 [03:14<00:00,  3.03it/s]\n",
      "100%|██████████| 588/588 [03:01<00:00,  3.23it/s]\n",
      "100%|██████████| 588/588 [03:07<00:00,  3.14it/s]\n",
      "100%|██████████| 588/588 [03:02<00:00,  3.22it/s]\n",
      "100%|██████████| 588/588 [04:00<00:00,  2.44it/s]\n",
      "100%|██████████| 588/588 [03:48<00:00,  2.57it/s]\n",
      "100%|██████████| 588/588 [04:06<00:00,  2.38it/s]\n",
      "100%|██████████| 588/588 [06:10<00:00,  1.59it/s]\n",
      "100%|██████████| 588/588 [04:47<00:00,  2.05it/s]\n",
      "100%|██████████| 588/588 [03:19<00:00,  2.95it/s]\n",
      "100%|██████████| 588/588 [03:07<00:00,  3.14it/s]\n",
      "100%|██████████| 588/588 [03:10<00:00,  3.09it/s]\n",
      "100%|██████████| 588/588 [03:09<00:00,  3.10it/s]\n",
      "100%|██████████| 588/588 [03:07<00:00,  3.13it/s]\n",
      "100%|██████████| 588/588 [03:14<00:00,  3.02it/s]\n",
      "100%|██████████| 588/588 [03:15<00:00,  3.00it/s]\n",
      "100%|██████████| 588/588 [03:08<00:00,  3.12it/s]\n",
      "100%|██████████| 588/588 [03:05<00:00,  3.17it/s]\n",
      "100%|██████████| 588/588 [03:03<00:00,  3.21it/s]\n",
      "100%|██████████| 588/588 [03:07<00:00,  3.13it/s]\n",
      "100%|██████████| 588/588 [03:04<00:00,  3.18it/s]\n",
      "100%|██████████| 588/588 [03:07<00:00,  3.14it/s]\n",
      "100%|██████████| 588/588 [04:09<00:00,  2.36it/s]\n",
      "100%|██████████| 588/588 [04:35<00:00,  2.14it/s]\n",
      "100%|██████████| 588/588 [03:52<00:00,  2.53it/s]\n",
      "100%|██████████| 588/588 [03:25<00:00,  2.86it/s]\n",
      "100%|██████████| 588/588 [03:07<00:00,  3.14it/s]\n",
      "100%|██████████| 588/588 [03:05<00:00,  3.18it/s]\n",
      "100%|██████████| 588/588 [03:08<00:00,  3.13it/s]\n",
      "100%|██████████| 588/588 [03:09<00:00,  3.11it/s]\n"
     ]
    }
   ],
   "source": [
    "for filename in all_filenames:\n",
    "    destination = filename.replace(\"predictions\", \"results/ontology_annotations\").replace(\"_pred\", \"_onto_concepts\")\n",
    "    f = open(os.path.join(dir_path, filename), \"r\", encoding = \"utf8\")\n",
    "    fr_sent = [line.strip() for line in f.readlines()]\n",
    "    f.close()\n",
    "    annotations_fr = []\n",
    "    for sentence in tqdm(fr_sent): \n",
    "        sentence = sentence.replace(\"%\", \" pour cent \")\n",
    "        annotations_per_sentence = []\n",
    "        annotations = get_json(REST_URL + \"/annotator?text=\" + urllib.parse.quote(sentence) + PREFERENCE_STRING)\n",
    "        found = set()\n",
    "        for result in annotations:\n",
    "            tag_loc = (result[\"annotations\"][0][\"from\"], result[\"annotations\"][0][\"to\"]) \n",
    "            if (tag_loc not in found):\n",
    "                info = (result[\"annotatedClass\"][\"prefLabel\"], len(result[\"annotations\"])) #Take a count of concepts across the sentence\n",
    "                annotations_per_sentence.append(info)\n",
    "                found.add(tag_loc)\n",
    "        annotations_fr.append(annotations_per_sentence)\n",
    "    sent_IDs = []\n",
    "    concepts = []\n",
    "    counts = []\n",
    "    for i in range(len(annotations_fr)):\n",
    "        for annotation_info in annotations_fr[i]:\n",
    "            sent_IDs.append(i)\n",
    "            concepts.append(annotation_info[0])\n",
    "            counts.append(annotation_info[1])\n",
    "    term_list = pd.DataFrame(data = {\"sent_ID\" : sent_IDs, \"concept\" : concepts, \"count\" : counts})\n",
    "    term_list.to_csv(destination, sep = \"\\t\", header = True, index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6286a2ff-fd0d-4526-890d-afa233511823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, repeat this workflow for the TFIDF model predictions\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "supp_path = \"../predictions/\"\n",
    "supp_files = [join(supp_path, f) for f in listdir(supp_path) if isfile(join(supp_path, f))] #4 files\n",
    "dir_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3ab81ee-fdea-452f-bcb4-744375f34610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../predictions/opus_base_AoN_tfidf_wce_unsampled_pred.txt',\n",
       " '../predictions/opus_base_simple_tfidf_wce_pred.txt',\n",
       " '../predictions/opus_big_fine_tfidf_wce_unsampled_pred.txt',\n",
       " '../predictions/opus_big_simple_tfidf_wce_unsampled_pred.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supp_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3413029e-4a28-4522-b9bf-b0433dba1adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [02:50<00:00,  3.44it/s]\n",
      "100%|██████████| 588/588 [02:40<00:00,  3.66it/s]\n",
      "100%|██████████| 588/588 [02:41<00:00,  3.63it/s]\n",
      "100%|██████████| 588/588 [02:45<00:00,  3.56it/s]\n"
     ]
    }
   ],
   "source": [
    "for filename in supp_files:\n",
    "    destination = filename.replace(\"predictions\", \"results/ontology_annotations\").replace(\"_pred\", \"_onto_concepts\")\n",
    "    f = open(os.path.join(dir_path, filename), \"r\", encoding = \"utf8\")\n",
    "    fr_sent = [line.strip() for line in f.readlines()]\n",
    "    f.close()\n",
    "    annotations_fr = []\n",
    "    for sentence in tqdm(fr_sent): \n",
    "        sentence = sentence.replace(\"%\", \" pour cent \")\n",
    "        annotations_per_sentence = []\n",
    "        annotations = get_json(REST_URL + \"/annotator?text=\" + urllib.parse.quote(sentence) + PREFERENCE_STRING)\n",
    "        found = set()\n",
    "        for result in annotations:\n",
    "            tag_loc = (result[\"annotations\"][0][\"from\"], result[\"annotations\"][0][\"to\"]) \n",
    "            if (tag_loc not in found):\n",
    "                info = (result[\"annotatedClass\"][\"prefLabel\"], len(result[\"annotations\"])) #Take a count of concepts across the sentence\n",
    "                annotations_per_sentence.append(info)\n",
    "                found.add(tag_loc)\n",
    "        annotations_fr.append(annotations_per_sentence)\n",
    "    sent_IDs = []\n",
    "    concepts = []\n",
    "    counts = []\n",
    "    for i in range(len(annotations_fr)):\n",
    "        for annotation_info in annotations_fr[i]:\n",
    "            sent_IDs.append(i)\n",
    "            concepts.append(annotation_info[0])\n",
    "            counts.append(annotation_info[1])\n",
    "    term_list = pd.DataFrame(data = {\"sent_ID\" : sent_IDs, \"concept\" : concepts, \"count\" : counts})\n",
    "    term_list.to_csv(destination, sep = \"\\t\", header = True, index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cae3d8-38f6-49a0-80d8-3a63472a23d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
