{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b812f08-a1d0-4b0d-9fab-1a6750534171",
   "metadata": {},
   "source": [
    "# Terminology Tagging for Choi et al. (2022)'s Soft Constraint Method\n",
    "\n",
    "We've obtained our noun alignments for 15% of our training set, and we will now determine the intersection of our filtered glossary and these nouns. Unfortunately, this will take quite a while (about 11 hours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6f52e40-c401-4462-b13f-eed26e0db3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "#Converts data in src [TAB] tgt [NEWLINE] format to a format suitable for model training\n",
    "def convertToDictFormat(data):\n",
    "    source = []\n",
    "    target = []\n",
    "    for example in data:\n",
    "        example = example.strip()\n",
    "        sentences = example.split(\"\\t\")\n",
    "        source.append(sentences[0])\n",
    "        target.append(sentences[1])\n",
    "    ready = Dataset.from_dict({\"en\":source, \"fr\":target})\n",
    "    return ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b4c604-1700-435c-b6a1-bcee9b7a4573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (C:/Users/ethan/.cache/huggingface/datasets/ethansimrm___text/ethansimrm--choi_filtered_cleaned_glossary-55c8ab5554eb133f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    }
   ],
   "source": [
    "#Load in clean glossary and convert to Dataset object\n",
    "glossary_terms = load_dataset(\"ethansimrm/choi_filtered_cleaned_glossary\", split = \"train\")\n",
    "terms_ready = convertToDictFormat(glossary_terms['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a99460ce-fdd5-42f2-b0bc-5a819f6286b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in noun alignments\n",
    "import pandas as pd\n",
    "noun_alignments_df = pd.read_csv(\"noun-alignments.txt\", sep = \"\\t\", header = None, names = [\"sent_ID\", \"en\", \"fr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81778005-4406-4c8a-b591-338d112f2883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take intersection by query\n",
    "def check_term_pair_in_glossary(en_term, fr_term):\n",
    "    for gloss_term in terms_ready:\n",
    "        #Only look for exact matches due to casing differences (glossary accounts for this)\n",
    "        if ((en_term == gloss_term[\"en\"]) and (fr_term == gloss_term[\"fr\"])):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dbc25e-5801-4b8f-9bce-084c86da8469",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_alignments_df['in_glossary'] = noun_alignments_df.apply(lambda row: check_term_pair_in_glossary(row['en'], row['fr']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cfe1f021-2c40-435e-bdc8-6a151eded33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will filter this later - for now, just check whether a term is present in our glossary, and output the result.\n",
    "noun_alignments_df.to_csv('found_nouns.txt', sep=\"\\t\", header = False, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
