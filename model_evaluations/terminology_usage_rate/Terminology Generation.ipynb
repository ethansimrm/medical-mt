{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ae10a3-d2a1-4820-b819-2be0610c4bdb",
   "metadata": {},
   "source": [
    "## Terminology Generation\n",
    "\n",
    "The aim here is to build a list of candidate terminologies from a dataset, when no external dictionaries are available. This allows us to generate an independent terminology baseline for our terminology usage rate (i.e., terminology recall) metric. By doing this, we are independent from dictionaries used in training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "650d7700-f75d-40eb-9365-0d47f7869dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "dir_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f938b5-eb56-482d-9076-19a06e1251d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open our test files first\n",
    "f = open(os.path.join(dir_path, \"../wmt22test.txt\"), \"r\", encoding = \"utf8\")\n",
    "en_sent = [line.strip() for line in f.readlines()]\n",
    "f.close()\n",
    "f = open(os.path.join(dir_path, \"../wmt22gold.txt\"), \"r\", encoding = \"utf8\")\n",
    "fr_sent = [line.strip() for line in f.readlines()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff742bf-0010-422b-8b7b-0b1a79e89b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in our PoS taggers - we choose transformers for accuracy in tokenisation and PoS-tagging - we are working on the \"gold standard\" after all.\n",
    "import spacy\n",
    "en_tagger = spacy.load(\"en_core_web_trf\")\n",
    "fr_tagger = spacy.load(\"fr_dep_news_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e48d4d41-1ead-45f5-9d64-adb95f134eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [00:35<00:00, 16.56it/s]\n"
     ]
    }
   ],
   "source": [
    "#We know that medical terminology mostly comprises nouns and adjectives. This is where Part-of-speech tagging comes in!\n",
    "#Removing conjugated verbs decreases noise, too - there are many forms due to masc/fem/plural etc. But first, let's tokenise and PoS-tag our sentences in line with Choi et al. (2022).\n",
    "en_tagged = []\n",
    "for sentence in tqdm(en_sent):\n",
    "    en_tagged_sent = en_tagger(sentence)\n",
    "    en_tagged_tokenised = [token for token in en_tagged_sent] #Access PoS tag information later on\n",
    "    en_tagged.append(en_tagged_tokenised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07b99e9f-a31d-4ad4-9d31-96a601851666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [00:43<00:00, 13.43it/s]\n"
     ]
    }
   ],
   "source": [
    "fr_tagged = []\n",
    "for sentence in tqdm(fr_sent):\n",
    "    fr_tagged_sent = fr_tagger(sentence)\n",
    "    fr_tagged_tokenised = [token for token in fr_tagged_sent]\n",
    "    fr_tagged.append(fr_tagged_tokenised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8ecfbe3-f9d6-480d-8ffa-c28920993618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-06-24 22:29:41,327 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: bert-base-multilingual-cased\n"
     ]
    }
   ],
   "source": [
    "#Now, let's perform word alignments. We couldn't use this to filter data because it does not output probabilities, but we can use this to compute word alignments.\n",
    "#Terminologies must be translated, so we expect good word alignments for them.\n",
    "from simalign import SentenceAligner\n",
    "aligner = SentenceAligner(matching_methods=\"a\") #Argmax only; the simAlign paper stated that this gives the best performance for English-French alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "129e0351-90d4-41ce-a9e9-778ea247c0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [01:21<00:00,  7.18it/s]\n"
     ]
    }
   ],
   "source": [
    "#We will pull out the token text, align that, then check the PoS information.\n",
    "alignments_list = []\n",
    "for i in tqdm(range(len(en_tagged))):\n",
    "    src_sent = [token.text for token in en_tagged[i]]\n",
    "    tgt_sent = [token.text for token in fr_tagged[i]] \n",
    "    alignments = aligner.get_word_aligns(src_sent, tgt_sent)\n",
    "    alignments_list.append(alignments[\"inter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b60412b-122d-47ad-9cf9-370cae981111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string #Need to filter out punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e1cc6e8-9726-4ff8-ba60-89cee8d00dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We only want NOUN-NOUN and ADJ-ADJ alignments, so let's save those and inspect them.\n",
    "output = open(\"candidate_terminology.txt\", \"w\", encoding = \"utf8\")\n",
    "for i in range(len(alignments_list)):\n",
    "    for aligned_pair in alignments_list[i]:\n",
    "        src_ind = aligned_pair[0]\n",
    "        tgt_ind = aligned_pair[1]\n",
    "        src_tok = en_tagged[i][src_ind]\n",
    "        tgt_tok = fr_tagged[i][tgt_ind]\n",
    "        if (((src_tok.pos_ == \"NOUN\") and (tgt_tok.pos_ == \"NOUN\"))) or (((src_tok.pos_ == \"ADJ\") and (tgt_tok.pos_ == \"ADJ\"))):\n",
    "            if not ((src_tok.text in string.punctuation) or (tgt_tok.text in string.punctuation)): #Ignore punctuation; these aren't terminology because they are found everywhere, e.g., %\n",
    "                output.write(str(i) + \"\\t\" + src_tok.text + \"\\t\" + tgt_tok.text + \"\\n\") #We include sentence numbers so we can check terminology for every sentence in the test set.\n",
    "output.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6b470f0-3fda-41f4-8d26-f8c7c36c9f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "candidates = pd.read_csv(\"candidate_terminology.txt\", sep = \"\\t\", header = None, names = [\"sent_ID\", \"en\", \"fr\"])\n",
    "candidates = candidates.drop_duplicates().reset_index() #For some reason there are duplicates, possibly due to SPACY errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c22846af-b63a-43ce-a365-244fcbb1afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#So far, so good, but we have a few misalignments (unavoidable due to test set misalignments) and some general vocabulary sprinkled in, e.g., man/woman, etc.\n",
    "#We don't need to accord general vocabulary the same importance as medical terminology. This means that we should filter based on general-domain frequency, because terminology is rare.\n",
    "#Let's see how many words are captured using this approach, and what sort of words they are.\n",
    "most_common_fr = pd.read_excel(\"Lexique_FR_PoS.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0a73c0f-e454-4ee5-be70-23c7545c1220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's begin by checking the frequency of each word in Lexique. \n",
    "def word_frequency(row):\n",
    "    query = row[\"fr\"].lower() #All our words are in lowercase\n",
    "    ans = most_common_fr[most_common_fr[\"Word\"] == query] #Search for word or lemma match\n",
    "    if (ans.empty):\n",
    "        ans = most_common_fr[most_common_fr[\"lemme\"] == query]\n",
    "        if (ans.empty):\n",
    "            return -1\n",
    "    if (len(ans) == 1):\n",
    "        return ans[\"freqfilms2\"].values[0]\n",
    "    else:\n",
    "        return ans[\"freqfilms2\"].max() #If we have more than one match, assume the most frequent form (due to plurals, etc.) - we are looking for the prevalence of this concept in the general domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e73b438-8435-432b-9fc9-4bd1e710138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates['frequency'] = candidates.apply(word_frequency, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd735818-d779-4755-8448-4cccf9ae33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#candidates_sorted = candidates.sort_values(by=['frequency']).reset_index()\n",
    "#Upon inspection, it is very difficult to make a clean cut, because there are some general terms which aren't in our general-domain corpus at all(!), like months = mois. \n",
    "#Conversely, there are some medical terms, like \"medicine\", which are very frequent. We need to compromise - what do we consider terminology?\n",
    "#Park et al. (2002) state that the domain-specificity of a term is given by its probability of occurrence in a domain-specific corpus divided by its occurrence in a general corpus.\n",
    "#Normalising for corpus length, a domain-specific term should occur far more often in the test set compared to a general corpus.\n",
    "#Thankfully, we have tokenised the test set, which also rids us of the articles (l', d', etc.), allowing easier comparison. The downside is that puntuation comes in, so we must account for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c63cbe6f-d48f-4b79-ac1f-766f5c1f9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_the_words = []\n",
    "word_count = 0;\n",
    "for sentence in fr_tagged:\n",
    "    for word in sentence:\n",
    "        if (word.text not in string.punctuation):\n",
    "            all_the_words.append(word.text.lower()) #Avoid capitalisation issues\n",
    "            word_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ba9c068-812d-4204-bc0e-c08896310b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15476"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count #15476 non-punctuation tokens, i.e., \"words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16e9c357-fd61-439c-99c1-dc37094db5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "in_domain_counts = Counter(all_the_words) #Frequency of all the words in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8d0d43f-39d9-4750-b4a1-16eacf42b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_domain_freq(row):\n",
    "    query = row[\"fr\"].lower() #All our words are in lowercase\n",
    "    return (in_domain_counts[query] / word_count) * 1000000 #We were given frequency per 1000000 words for the general corpus, so we must upscale to get frequency per \"million words\" here too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16288797-7155-454d-910c-6abee8daa16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates['ID_frequency'] = candidates.apply(in_domain_freq, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "113f31ba-5fc9-455b-a681-f309b9179004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "candidates[\"domain_specificity\"] = np.log(candidates[\"ID_frequency\"] / (candidates[\"frequency\"] + 1.01)) #Account for negative and zero values, and compress the scale because our values are large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "389d9368-574a-4c13-b89b-cbe912cebe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_sorted = candidates.sort_values(by=['domain_specificity']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf426937-77a7-4516-8365-e4a5de4ad94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "junk = candidates_sorted[candidates_sorted[\"domain_specificity\"] < 3] #Justifies choice of 3 as a conservative cutoff - most of these words are non-terminological.\n",
    "output = open(\"non_term_list.txt\", \"w\", encoding = \"utf8\")\n",
    "for i in range(len(junk[\"en\"])):\n",
    "    output.write(str(junk[\"sent_ID\"][i]) + \"\\t\" + junk[\"en\"][i] + \"\\t\" + junk[\"fr\"][i] +  \"\\n\")\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c89f5ba4-59c7-468f-839a-75943b5f7179",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_filtered = candidates_sorted[candidates_sorted[\"domain_specificity\"] >= 3].sort_values(by = [\"sent_ID\"]).reset_index(drop=True)\n",
    "#3 is a conservative cutoff - there are a few non-terminology words here, but not that many compared to < 3. This aims to be a superset of a human-annotated terminology anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02cab15c-2d4f-479d-9996-b66208fd6272",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_filtered_counts = candidates_filtered[[\"sent_ID\", \"fr\"]]\n",
    "candidates_filtered_counts = candidates_filtered_counts.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58519e8f-a23b-4204-a90f-88fb3d8b3262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_count_in_sentence(row):\n",
    "    query = row[\"fr\"].lower()\n",
    "    return len([found for found in fr_tagged[row[\"sent_ID\"]] if query == found.text.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cbd6ba7a-82b0-4773-968c-9fa5e56baa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_filtered_counts[\"count\"] = candidates_filtered_counts.apply(find_count_in_sentence, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9759e30-eafd-4e23-a20f-de8dd5340ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open(\"wmt22gold_terminology.txt\", \"w\", encoding = \"utf8\")\n",
    "for i in range(len(candidates_filtered_counts[\"fr\"])):\n",
    "    output.write(str(candidates_filtered_counts[\"sent_ID\"][i]) + \"\\t\" + candidates_filtered_counts[\"fr\"][i].lower() + \"\\t\" + str(candidates_filtered_counts[\"count\"][i]) + \"\\n\")\n",
    "output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
