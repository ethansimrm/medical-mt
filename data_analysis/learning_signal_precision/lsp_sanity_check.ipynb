{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ea9e58-db04-450c-8996-beec9b9bda10",
   "metadata": {},
   "source": [
    "# Sanity Check\n",
    "Just to check that we've got the same number of tokens when we tokenise our LSP words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2429be5-5f66-405c-bd2b-63a721138988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "base_tok = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "big_tok = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "108a0775-1a2c-438d-8a8d-a2a5d8dad96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (C:/Users/ethan/.cache/huggingface/datasets/ethansimrm___text/ethansimrm--sampled_glossary_1.0_train-2f618074f2b08fee/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "#Converts data in src [TAB] tgt [NEWLINE] format to a format suitable for model training\n",
    "def convertToDictFormat(data):\n",
    "    source = []\n",
    "    target = []\n",
    "    for example in data:\n",
    "        example = example.strip()\n",
    "        sentences = example.split(\"\\t\")\n",
    "        source.append(sentences[0])\n",
    "        target.append(sentences[1])\n",
    "    ready = Dataset.from_dict({\"en\":source, \"fr\":target})\n",
    "    return ready\n",
    "terms_in_train = load_dataset(\"ethansimrm/sampled_glossary_1.0_train\", split = \"train\")\n",
    "terms_in_train_ready = convertToDictFormat(terms_in_train['text'])\n",
    "input = open(\"train_word_frequencies.txt\", \"r\", encoding=\"utf8\")\n",
    "words = []\n",
    "counts = []\n",
    "for line in input.readlines():\n",
    "    line_list = line.strip().split(\"\\t\")\n",
    "    words.append(line_list[0])\n",
    "    counts.append(int(line_list[1]))\n",
    "input.close()\n",
    "train_word_freq = pd.DataFrame(data = {\"word\":words, \"count\":counts}) #We could not use pandas' read_csv due to \" being a token.\n",
    "present = train_word_freq[train_word_freq[\"word\"].isin(terms_in_train_ready['fr'])]\n",
    "words_only = present[\"word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db83a5f6-34e4-418d-b91d-f259b4acca8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1406"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1ab5d60-3018-493b-9aa2-31105d41f573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5           chromosomique\n",
       "87                 grippe\n",
       "108             anticorps\n",
       "112             détection\n",
       "184            directives\n",
       "               ...       \n",
       "277240          Inférence\n",
       "279574                Cor\n",
       "283448      gatifloxacine\n",
       "284551    Biostatistiques\n",
       "285644           Parasite\n",
       "Name: word, Length: 1406, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0920fedb-82fc-4404-850d-783f0f9c4b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_only.to_csv(\"terms_for_lsp.txt\", sep = \"\\n\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be4b42d4-fbc5-40d5-bece-435eee14e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_BASE_TOKENS = [0, 1, 59513]\n",
    "token_list = []\n",
    "parent_word = []\n",
    "counts = []\n",
    "for row in present.itertuples():\n",
    "    token_ids = base_tok(text_target=row.word)[\"input_ids\"]\n",
    "    for id in token_ids:\n",
    "        if (id not in SPECIAL_BASE_TOKENS):\n",
    "            token_list.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce575331-46fa-4912-b79f-94b6b957472f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1695"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "len(Counter(token_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
