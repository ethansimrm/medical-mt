{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9853c1ed-82d2-4702-b2cd-7cb4a15a1fe5",
   "metadata": {},
   "source": [
    "# Result Generation\n",
    "\n",
    "This abstracts the core of the former test_results.py script, and outputs scores for the four fluency metrics and our ontology detection metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fccd14a-e352-43c6-a5a2-13f5c780f625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ethan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ethan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ethan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "#The four metrics we will evaluate\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "ter = evaluate.load(\"ter\")\n",
    "meteor = evaluate.load('meteor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5234bb89-542e-45ff-b90c-c7d7c0a52dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, repeat this workflow for all predictions we've gathered so far - 32 of them\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "base_path = \"predictions/opus_en_fr_base/\"\n",
    "big_path = \"predictions/opus_en_fr_big/\"\n",
    "base_files = [join(base_path, f) for f in listdir(base_path) if isfile(join(base_path, f))] #16 files, each comprising all our term predictions\n",
    "big_files = [join(big_path, f) for f in listdir(big_path) if isfile(join(big_path, f))]\n",
    "all_filenames = base_files + big_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "790d7589-3e2a-453c-9106-1b334a5d7fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentences(filename):\n",
    "    f = open(filename, \"r\", encoding = \"utf8\")\n",
    "    sentences = [line.strip() for line in f.readlines()]\n",
    "    f.close()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f510a2a3-136a-4a13-8dd5-e04cddd11922",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENCE_FILE = \"wmt22gold.txt\"\n",
    "ONTO_FILE = \"wmt22gold_onto_concepts.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13fb6255-cfc9-42d6-9689-354d6f7f1ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_tag(pred_counts, ref_counts): #Helps us to compute summary statistics later\n",
    "    tags = []\n",
    "    true_positives = []\n",
    "    false_positives = []\n",
    "    false_negatives = []\n",
    "    for (idx, count) in enumerate(pred_counts):\n",
    "        if (pred_counts[idx] == 0):\n",
    "            tags.append(\"MISSED\")\n",
    "            true_positives.append(0) #We are missing this number of concepts\n",
    "            false_positives.append(0)\n",
    "            false_negatives.append(ref_counts[idx])\n",
    "        elif (ref_counts[idx] == 0):\n",
    "            tags.append(\"EXTRANEOUS\")\n",
    "            true_positives.append(0) #We have overpredicted this number of concepts\n",
    "            false_positives.append(pred_counts[idx])\n",
    "            false_negatives.append(0)\n",
    "        else:\n",
    "            tags.append(\"RECOVERED\")\n",
    "            true_positives.append(min(pred_counts[idx], ref_counts[idx])) #If we predict too many, report a positive number. If we predict too few, report a negative number.\n",
    "            discrepancy = pred_counts[idx] - ref_counts[idx]\n",
    "            if (discrepancy > 0): #Overpredicted\n",
    "                false_positives.append(discrepancy)\n",
    "                false_negatives.append(0)\n",
    "            elif (discrepancy < 0): #Underpredicted\n",
    "                false_positives.append(0)\n",
    "                false_negatives.append(abs(discrepancy))\n",
    "            else:\n",
    "                false_positives.append(0)\n",
    "                false_negatives.append(0)\n",
    "    return (true_positives, false_positives, false_negatives, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cdd4018b-64b9-46fb-a047-43bc6a0c2d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [02:54<00:00,  5.44s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for filename in tqdm(all_filenames):\n",
    "\n",
    "    #Compose filenames\n",
    "    destination_metrics = filename.replace(\"predictions\", \"results\").replace(\"_pred\", \"_results\")\n",
    "    destination_onto_table = filename.replace(\"predictions\", \"results\").replace(\"_pred\", \"_onto_output\")\n",
    "    predicted_onto = filename.replace(\"predictions\", \"results/ontology_annotations\").replace(\"_pred\", \"_onto_concepts\")\n",
    "\n",
    "    #Read in data\n",
    "    preds = getSentences(filename)\n",
    "    onto_preds = pd.read_csv(predicted_onto, sep = \"\\t\")\n",
    "    refs = getSentences(REFERENCE_FILE)\n",
    "    onto_refs = pd.read_csv(ONTO_FILE, sep = \"\\t\")\n",
    "\n",
    "    #Scan for duplicates and handle them if they made it through our initial filter - rare edge case where the first annotated occurrence is not detected by a subsequent ontology\n",
    "    #but subsequent occurrences are detected\n",
    "    #In this case, we know that the duplicated row has fewer counts than the first occurrence, so we can drop it safely. The fact of the matter is that \"Poids\" was detected twice.\n",
    "    #onto_preds = onto_preds[[\"sent_ID\", \"concept\", \"count\"]]\n",
    "    #onto_preds = onto_preds.drop_duplicates(subset=[\"sent_ID\", \"concept\"], ignore_index = True).reset_index(drop=True)\n",
    "    #onto_preds.to_csv(predicted_onto, sep = \"\\t\", header = True, index = False) #Write back to source\n",
    "\n",
    "    #Full outer join enables easy detection of matches, extraneous predictions, and misses\n",
    "    aggregated_onto = onto_preds.merge(onto_refs, how = \"outer\", on = [\"sent_ID\", \"concept\"], \n",
    "                                       suffixes = [\"_preds\", \"_refs\"]).fillna(0).astype({\"count_preds\": int, \"count_refs\": int})  #NaNs just mean either extraneous or missed\n",
    "    aggregated_onto[\"true_positives\"], aggregated_onto[\"false_positives\"], aggregated_onto[\"false_negatives\"], aggregated_onto[\"remark\"] = assign_tag(aggregated_onto[\"count_preds\"], \n",
    "                                                                                                                                                      aggregated_onto[\"count_refs\"])\n",
    "    \n",
    "    #Log table for human interpretation\n",
    "    aggregated_onto.to_csv(destination_onto_table, sep = \"\\t\", index = False)\n",
    "    \n",
    "    #Compute summary statistics for ontology prediction\n",
    "    total_ref_concepts = sum(aggregated_onto[\"count_refs\"])\n",
    "    total_pred_concepts = sum(aggregated_onto[\"count_preds\"])\n",
    "    TP = sum(aggregated_onto[\"true_positives\"])\n",
    "    FP = sum(aggregated_onto[\"false_positives\"])\n",
    "    FN = sum(aggregated_onto[\"false_negatives\"])\n",
    "    recall = TP / (TP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    #Compute fluency metrics and write everything out\n",
    "    output = open(destination_metrics, \"w\", encoding = \"utf8\")\n",
    "    output.write(\"Fluency Metrics\\n\\n\")\n",
    "    result = bleu.compute(predictions = preds, references = refs) #BLEU score for provided input and references\n",
    "    output.write(\"BLEU: \" + str(result[\"score\"]) + \"\\n\")\n",
    "    result = chrf.compute(predictions = preds, references = refs, word_order = 2) #Include word bigrams for CHRF++\n",
    "    output.write(\"CHRF++: \" + str(result[\"score\"]) + \"\\n\")\n",
    "    result = ter.compute(predictions = preds, references = refs, case_sensitive = True) #Casing is important - treat as an edit error\n",
    "    output.write(\"TER: \" + str(result[\"score\"]) + \"\\n\")\n",
    "    result = meteor.compute(predictions = preds, references = refs)\n",
    "    output.write(\"METEOR: \" + str(result[\"meteor\"]) + \"\\n\")\n",
    "    output.write(\"\\nOntology Prediction Metrics\\n\\n\")\n",
    "    output.write(\"Total Concepts Present: \" + str(total_ref_concepts) + \"\\n\")\n",
    "    output.write(\"Total Concepts Predicted: \" + str(total_pred_concepts) + \"\\n\")\n",
    "    output.write(\"Recall: \" + str(recall) + \"\\n\")\n",
    "    output.write(\"Precision: \" + str(precision) + \"\\n\")\n",
    "    output.write(\"F1 Score: \" + str(f1) + \"\\n\")\n",
    "    output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
