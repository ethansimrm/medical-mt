{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5643d576-2a79-4074-a8dc-11b5ce04a8bb",
   "metadata": {},
   "source": [
    "# Clinical Ontology Testing\n",
    "\n",
    "NER by itself isn't very reliable. Let's cross-reference this with the NCBO clinical ontologies, which include SNOMED-CT and many others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5460a2ac-268c-463b-aec2-fd3e7f73e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be09d2a8-f1a1-45ac-8fb1-f835f48792a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.getcwd()\n",
    "f = open(os.path.join(dir_path, \"../../wmt22test.txt\"), \"r\", encoding = \"utf8\")\n",
    "en_sent = [line.strip() for line in f.readlines()]\n",
    "f.close()\n",
    "f = open(os.path.join(dir_path, \"../../wmt22gold.txt\"), \"r\", encoding = \"utf8\")\n",
    "fr_sent = [line.strip() for line in f.readlines()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e6eeed3-f43d-4c70-a993-b095ecb42dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c371ce5ec18410da457230372ef4f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 22:03:23 INFO: Downloading these customized packages for language: fr (French)...\n",
      "==============================\n",
      "| Processor       | Package  |\n",
      "------------------------------\n",
      "| tokenize        | sequoia  |\n",
      "| mwt             | sequoia  |\n",
      "| pos             | sequoia  |\n",
      "| forward_charlm  | newswiki |\n",
      "| backward_charlm | newswiki |\n",
      "| pretrain        | conll17  |\n",
      "==============================\n",
      "\n",
      "2023-07-14 22:03:23 INFO: File exists: C:\\Users\\ethan\\stanza_resources\\fr\\tokenize\\sequoia.pt\n",
      "2023-07-14 22:03:23 INFO: File exists: C:\\Users\\ethan\\stanza_resources\\fr\\mwt\\sequoia.pt\n",
      "2023-07-14 22:03:23 INFO: File exists: C:\\Users\\ethan\\stanza_resources\\fr\\pos\\sequoia.pt\n",
      "2023-07-14 22:03:23 INFO: File exists: C:\\Users\\ethan\\stanza_resources\\fr\\forward_charlm\\newswiki.pt\n",
      "2023-07-14 22:03:23 INFO: File exists: C:\\Users\\ethan\\stanza_resources\\fr\\backward_charlm\\newswiki.pt\n",
      "2023-07-14 22:03:24 INFO: File exists: C:\\Users\\ethan\\stanza_resources\\fr\\pretrain\\conll17.pt\n",
      "2023-07-14 22:03:24 INFO: Finished downloading models and saved to C:\\Users\\ethan\\stanza_resources.\n",
      "2023-07-14 22:03:24 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372b5c08fbe647ceb17a140c25afcfd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 22:03:24 INFO: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | sequoia |\n",
      "| mwt       | sequoia |\n",
      "| pos       | sequoia |\n",
      "=======================\n",
      "\n",
      "2023-07-14 22:03:24 INFO: Using device: cpu\n",
      "2023-07-14 22:03:24 INFO: Loading: tokenize\n",
      "2023-07-14 22:03:24 INFO: Loading: mwt\n",
      "2023-07-14 22:03:24 INFO: Loading: pos\n",
      "2023-07-14 22:03:25 INFO: Done loading processors!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4d3e6bd7fa4e35b8502f8dd8f2b7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 22:03:25 WARNING: Can not find mwt: mimic from official model list. Ignoring it.\n",
      "2023-07-14 22:03:25 INFO: Downloading these customized packages for language: en (English)...\n",
      "=============================\n",
      "| Processor       | Package |\n",
      "-----------------------------\n",
      "| tokenize        | mimic   |\n",
      "| pos             | mimic   |\n",
      "| pretrain        | mimic   |\n",
      "| forward_charlm  | mimic   |\n",
      "| backward_charlm | mimic   |\n",
      "=============================\n",
      "\n",
      "2023-07-14 22:03:25 INFO: File exists: C:\\Users\\ethan\\stanza_resources\\en\\tokenize\\mimic.pt\n",
      "2023-07-14 22:03:25 INFO: File exists: C:\\Users\\ethan\\stanza_resources\\en\\pos\\mimic.pt\n",
      "2023-07-14 22:03:25 INFO: File exists: C:\\Users\\ethan\\stanza_resources\\en\\pretrain\\mimic.pt\n",
      "2023-07-14 22:03:25 INFO: File exists: C:\\Users\\ethan\\stanza_resources\\en\\forward_charlm\\mimic.pt\n",
      "2023-07-14 22:03:26 INFO: File exists: C:\\Users\\ethan\\stanza_resources\\en\\backward_charlm\\mimic.pt\n",
      "2023-07-14 22:03:26 INFO: Finished downloading models and saved to C:\\Users\\ethan\\stanza_resources.\n",
      "2023-07-14 22:03:26 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6faa31a96647f984a221af307a6133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 22:03:26 WARNING: Can not find mwt: mimic from official model list. Ignoring it.\n",
      "2023-07-14 22:03:26 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | mimic   |\n",
      "| pos       | mimic   |\n",
      "=======================\n",
      "\n",
      "2023-07-14 22:03:26 INFO: Using device: cpu\n",
      "2023-07-14 22:03:26 INFO: Loading: tokenize\n",
      "2023-07-14 22:03:26 INFO: Loading: pos\n",
      "2023-07-14 22:03:26 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#We will obtain annotations using get_wmt22_onto_annotations.py. For now, tokenise and PoS-tag both EN and FR sentences.\n",
    "import stanza\n",
    "\n",
    "stanza.download('fr', processors='tokenize, mwt, pos', package='sequoia')\n",
    "nlp_fr = stanza.Pipeline('fr', processors='tokenize, mwt, pos', package='sequoia')\n",
    "stanza.download('en', processors='tokenize, pos', package='mimic')\n",
    "nlp_en = stanza.Pipeline('en', processors='tokenize, pos', package='mimic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dae5e5b2-bce5-4db8-9b61-6d42fefab8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [04:24<00:00,  2.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "#Extract only the information we need - PoS tags and text.\n",
    "fr_tagged = []\n",
    "for sentence in tqdm(fr_sent):\n",
    "    doc = nlp_fr(sentence)\n",
    "    tokens = [{\"text\" : word.text, \"upos\" : word.upos} for sent in doc.sentences for word in sent.words]\n",
    "    fr_tagged.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "540533f5-e1e7-4d57-badf-4172ea2cfab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [03:43<00:00,  2.63it/s]\n"
     ]
    }
   ],
   "source": [
    "en_tagged = []\n",
    "for sentence in tqdm(en_sent):\n",
    "    doc = nlp_en(sentence)\n",
    "    tokens = [{\"text\" : word.text, \"upos\" : word.upos} for sent in doc.sentences for word in sent.words]\n",
    "    en_tagged.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb64f275-9873-41eb-ac2b-150b6f165c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-07-14 22:51:35,500 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: bert-base-multilingual-cased\n",
      "100%|██████████| 588/588 [01:25<00:00,  6.88it/s]\n"
     ]
    }
   ],
   "source": [
    "#Word alignment\n",
    "from simalign import SentenceAligner\n",
    "aligner = SentenceAligner(matching_methods=\"a\") #Argmax only; the simAlign paper stated that this gives the best performance for English-French alignment\n",
    "alignments_list = []\n",
    "for i in tqdm(range(len(en_tagged))):\n",
    "    src_sent = [token[\"text\"] for token in en_tagged[i]]\n",
    "    tgt_sent = [token[\"text\"] for token in fr_tagged[i]] \n",
    "    alignments = aligner.get_word_aligns(src_sent, tgt_sent)\n",
    "    alignments_list.append(alignments[\"inter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6dfeb50-bee7-456d-9f3e-8def15498e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6863/6863 [05:59<00:00, 19.11it/s]\n"
     ]
    }
   ],
   "source": [
    "#Read in our annotations, and word-tokenise any multi-word annotations with the same EN tokeniser to ensure consistency\n",
    "f = open(\"wmt22_onto_annotations.txt\", \"r\", encoding = \"utf8\")\n",
    "output = open(\"wmt22_onto_word_tok.txt\", \"w\", encoding = \"utf8\")\n",
    "for line in tqdm(f.readlines()):\n",
    "    word_list = line.strip().split(\"\\t\")\n",
    "    doc = nlp_en(word_list[1])\n",
    "    words = [word.text for sent in doc.sentences for word in sent.words]\n",
    "    for word in words:\n",
    "        output.write(word_list[0] + \"\\t\" + word + \"\\n\")\n",
    "f.close()\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da3766ca-0052-4978-a104-9def329109bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "anno_list = pd.read_csv(\"wmt22_onto_word_tok.txt\", sep = \"\\t\", header = None, names = [\"sent_ID\", \"annotation\"])\n",
    "anno_list = anno_list.drop_duplicates().reset_index(drop=True)\n",
    "anno_list[\"anno_lower\"] = anno_list[\"annotation\"].apply(str.lower)\n",
    "anno_list = anno_list.drop(columns = \"annotation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "354f903a-8b42-4ace-87ef-5410eeaa7df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have our word-tokenised annotation list, we will query our alignments list.\n",
    "import string\n",
    "\n",
    "prospective_ne_fr = []\n",
    "for i in range(len(alignments_list)):\n",
    "    ne_fr_per_sentence = []\n",
    "    #Filter annotations dataframe\n",
    "    sentence_annos = anno_list[anno_list[\"sent_ID\"] == i]\n",
    "    for alignment in alignments_list[i]:\n",
    "        src_word = en_tagged[i][alignment[0]]\n",
    "        tgt_word = fr_tagged[i][alignment[1]]\n",
    "        #If there is a match for our source word\n",
    "        if not (sentence_annos[sentence_annos[\"anno_lower\"] == src_word[\"text\"].lower()].empty):\n",
    "            if ((src_word[\"upos\"] in [\"PROPN\", \"NOUN\", \"ADJ\", \"VERB\"]) & #Determiners, adverbs, prepositions, etc. are not terminologies of interest, because we are doing word-level checking\n",
    "            (tgt_word[\"upos\"] == src_word[\"upos\"]) & #Ensure both pipelines agree on PoS tags\n",
    "            (src_word[\"text\"] not in string.punctuation) &\n",
    "            (tgt_word[\"text\"] not in string.punctuation)): #In case of incorrect tagging wrt punctuation\n",
    "                ne_fr_per_sentence.append(tgt_word[\"text\"])\n",
    "    prospective_ne_fr.append(ne_fr_per_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0ea1e7f8-672f-40a1-a44f-80cf15a7624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_IDs = []\n",
    "terms = []\n",
    "for i in range(len(prospective_ne_fr)):\n",
    "    for term in prospective_ne_fr[i]:\n",
    "        sent_IDs.append(i)\n",
    "        terms.append(term)\n",
    "term_list = pd.DataFrame(data = {\"sent_ID\" : sent_IDs, \"term\" : terms})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6ebd5948-9e0a-48ad-a3dc-8d4f4423a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_count_in_sentence_exact_match(row):\n",
    "    query = row[\"term\"]\n",
    "    return len([found for found in fr_tagged[row[\"sent_ID\"]] if query == found[\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "390593ef-e74d-407e-b2c1-65d43abb5fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_list[\"count\"] = term_list.apply(find_count_in_sentence_exact_match, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0510e098-1d0c-4fca-bafa-469174d1d6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_list = term_list.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ffeb7018-3b0c-4a90-9000-ca5efdd97358",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_list.to_csv(\"wmt22_onto_terms_counts.txt\", sep = \"\\t\", header = False, index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "77b5cf17-4ba3-47db-b2c5-4b490723f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we will generate a separate list without casing, and aggregate counts as usual. Interestingly, we have the same number of rows, indicating that there are no terminologies which \n",
    "#appear multiple times within a single sentence, but with different casing. This means that we can stop here for now.\n",
    "#term_list_uncased = term_list[[\"sent_ID\", \"term\"]]\n",
    "#term_list_uncased[\"uncased_term\"] = term_list_uncased[\"term\"].apply(str.lower)\n",
    "#term_list_uncased = term_list_uncased.drop(columns = \"term\")\n",
    "#term_list_uncased = term_list_uncased.drop_duplicates().reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
