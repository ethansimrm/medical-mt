{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5043605c-1ccf-483b-ba4c-2dc420963d10",
   "metadata": {},
   "source": [
    "## WMT OOD and Glossary Processing\n",
    "This notebook will be used for devising a script to parse WMT OOD data and our MeSpEn glossary.\n",
    "\n",
    "### Start State\n",
    "The WMT OOD data is in a big .tsv file about 110MB in size, while the MeSpEn glossary is in a small .tsv file containing around 6.5k rows.\n",
    "\n",
    "### Desired Outcome\n",
    "For both, we want to generate a single, **separate** .txt file containing aligned pairs of source and target sentences in the format `source_sentence (TAB) target_sentence (newline)`. This is more convenient for evaluation sets, for which we will use the HuggingFace `Trainer` API. We will concatenate them later. Note that we haven't started preprocessing our data - we will use Wu et al.(2022)'s method for this later.\n",
    "\n",
    "### Considerations\n",
    "- HuggingFace's programmatic Dataset upload API is broken; this issue is still unresolved and cannot be overcome by switching to earlier versions. We therefore have to preprocess and upload via the web interface.\n",
    "- We cannot use a CSV, because the `datasets` library's to_csv function adds strange characters to the data, even with the correct UTF encoding applied. In essence, converting to CSV and uploading _that_ leads to data corruption. Intuitively, we must use a .txt file and upload that via the web interface - end-users will load our dataset as a Dataset or DatasetDict object, and the .txt file source will be transparent to them. I'll write up a script to process these .txt files, since they will all be in the same format.\n",
    "- We won't be using pandas here, as our sentences are not split into multiple tab-separated columns (which pandas is good for). We'll just read the .txt files and process them as we go.\n",
    "\n",
    "### Unpacking\n",
    "We've already unzipped the WMT22 OOD data using `gunzip`. This yields a tab-separated file. Let's begin with the OOD data; it seems more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a62cdc-53f3-42c4-b4be-945cbeef2e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get current working directory\n",
    "import os\n",
    "dir_path = os.getcwd()\n",
    "PATHS = {\"OOD\":\"raw_training_data/wmt22_enfr_news_train.tsv\", \"glossary\":\"raw_training_data/mespen_enfr_glossary.tsv\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c6d1132-04a7-485a-9fe1-99287a9edd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_ood = open(os.path.join(dir_path, PATHS[\"OOD\"]), \"r\", encoding=\"utf8\") #Other encodings don't seem to work.\n",
    "output = open(os.path.join(dir_path, \"ood_train.txt\"), \"w\", encoding=\"utf8\")\n",
    "for line in wmt_ood.readlines():\n",
    "    if (line != \"\\t\\n\"): #There are blank lines between articles - we must remove them.\n",
    "        line = line.replace(\"�\", \"\") #There are some non-UTF8 characters which seem to occur at random intervals in a few sentences, \n",
    "        #replacing seemingly random characters (or even no characters). Unsure what these mean.\n",
    "        #If sentences don't make sense after doing this, we can eliminate such sentences (e.g., mismatched quotation marks) during pre-processing.\n",
    "        output.write(line) #Apart from that, it's pretty much in our ideal format! Yay!\n",
    "wmt_ood.close()\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2452c5f-4b48-4762-85d3-a59247a764d6",
   "metadata": {},
   "source": [
    "That wasn't too bad, although we may have a bit more work to do during preprocessing. Let's move on to the MeSpEn glossary. At first glance, this glossary is quite dirty - there are english phrases scattered all around the target side, abbreviation expansions rather than translations, partial translations, comments from the translator, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df6acbb6-fa35-4ac7-a030-2ff89fb9688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_list = [\",\", \";\", \"(\", \")\", \"!\", \"\\\\\", \"/\", \"#\", \"*\", \"[\", \"]\", \"=\"] #All tokens which deviate from one-source-term-to-one-target-term translation\n",
    "removed_sentences = [\"Abbreviation\tEntire Word\", \"Acute respiratory insufficiency\tPossibly no EN abbrev.\", \"Ao\tAorta\", \"chest pain\tretro-sternal pain\", \"Cx\tCircumflex\", \"Day 2\tDay 2\",\n",
    "                     \"English\tFrench\", \"French\tEnglish\", \"Hearing Aids\tãÚíäÇÊ ÓãÚíÉ\", \"hr\tHour\", \"nv\tNormal value\", \"Possibly no EN abbrev.\tAcute respiratory insufficiency\", \"wind-sock design\tref.\",\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "578c2cbd-94cc-4551-801c-44021ca20f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "glossary = open(os.path.join(dir_path, PATHS[\"glossary\"]), \"r\", encoding=\"utf8\")\n",
    "output1 = open(os.path.join(dir_path, \"clean_glossary.txt\"), \"w\", encoding = \"utf8\")\n",
    "output2 = open(os.path.join(dir_path, \"dirty_glossary.txt\"), \"w\", encoding = \"utf8\")\n",
    "banned_list = open(os.path.join(dir_path, \"banned.txt\"), \"w\", encoding = \"utf8\")\n",
    "for line in glossary.readlines():\n",
    "    line = line[3:] #Skip space, number, space preceding source terminology\n",
    "    line = line.strip() #Remove all leading and trailing spaces\n",
    "    if (any(sentence in line for sentence in removed_sentences)): #Filter out removed sentences\n",
    "        continue\n",
    "    line = line.replace(\".v.\", \"\") #These represent verbs; no such annotations will be present at inference time\n",
    "    line = line.replace(\"=>\", \"\") #These markings may be translator-specific\n",
    "    line = line.replace(\"->\", \"\") #These markings may be translator-specific\n",
    "    terms = line.split(\"\\t\")\n",
    "    if((terms[0].strip() == \"\") or (terms[1].strip() == \"\")): #Some lines are apparently empty\n",
    "        continue\n",
    "    line = terms[0].strip() + \"\\t\" + terms[1].strip() #Some lines contain extra spaces between the \\t symbols\n",
    "    if(terms[0].strip().isupper()): #Flag for removal due to abbreviation possibility - I will manually append later\n",
    "        banned_list.write(line + '\\n')\n",
    "        continue\n",
    "    if not (any(substring in line for substring in dirty_list)): #Filter out possibly problematic sentences\n",
    "        output1.write(line + '\\n')\n",
    "    output2.write(line + '\\n')\n",
    "banned_list.close()\n",
    "output1.close()\n",
    "output2.close()\n",
    "glossary.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562398f9-57a3-4cb8-9dde-c1a79622c210",
   "metadata": {},
   "source": [
    "It is very difficult to filter this, mainly because a, b TAB c may mean that both source terminologies a and b translate into c with equal fidelity, or that b is a contextualisation of a. We also cannot use langdetect or langid to filter this by language, because we only have terms, rather than sentences. Yet, we may lose important information if we only use the clean glossary (although 5k terms is still quite substantial - HW-TSC only had 6k). Thus, I will manually go through both clean and dirty glossaries to remove obvious English terminology on the target side - this is okay because they are small. We also know that the English terminology arrived due to the mistaken inclusion of an English abbreviation-to-entire word list, so we can filter based on that. Choi et al. managed to get good results by appending the glossary directly to the training corpus, and we can do that with the dirty glossary. What's important is that we only use one-to-one terms for our soft-constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd8aa37-39cb-43e4-b9e7-3e2b363eb32b",
   "metadata": {},
   "source": [
    "After manually filtering all lists, we add the acceptable pairs (i.e., non-English abbrev to English full form pairs) to the lists as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "75d38053-4450-466f-af38-659ff31a83d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_banned = open(os.path.join(dir_path, \"banned_filtered.txt\"), \"r\", encoding = \"utf8\")\n",
    "clean_glossary = open(os.path.join(dir_path, \"clean_glossary.txt\"), \"a\", encoding = \"utf8\")\n",
    "dirty_glossary = open(os.path.join(dir_path, \"dirty_glossary.txt\"), \"a\", encoding = \"utf8\")\n",
    "for line in filtered_banned.readlines():\n",
    "    line = line.strip()\n",
    "    if not (any(substring in line for substring in dirty_list)):\n",
    "        clean_glossary.write(line + '\\n')\n",
    "    dirty_glossary.write(line + '\\n')\n",
    "dirty_glossary.close()\n",
    "clean_glossary.close()\n",
    "filtered_banned.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
