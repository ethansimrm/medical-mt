{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2304685-007a-4efa-927f-9cf2fbce5179",
   "metadata": {},
   "source": [
    "# Named Entity Recognition Testing\n",
    "\n",
    "Previously, we used heuristic methods to identify terminology, but we can do better with named entity recognition. We will use Stanza, the best SOTA off-the-shelf NER algorithm for biomedical NER per Kühnel and Fluck (2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0fd25f7b-e6a6-469d-8b9f-7b8b86c5c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "dir_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "09735347-26f9-4d4f-b0ab-b65c12c1afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open our test files first\n",
    "f = open(os.path.join(dir_path, \"../../wmt22test.txt\"), \"r\", encoding = \"utf8\")\n",
    "en_sent = [line.strip() for line in f.readlines()]\n",
    "f.close()\n",
    "f = open(os.path.join(dir_path, \"../../wmt22gold.txt\"), \"r\", encoding = \"utf8\")\n",
    "fr_sent = [line.strip() for line in f.readlines()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4f32f7f9-f094-4bd7-a1aa-4b859675cd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc2c657c9cf439098882811ca69864a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 21:34:36 INFO: Downloading these customized packages for language: fr (French)...\n",
      "==============================\n",
      "| Processor       | Package  |\n",
      "------------------------------\n",
      "| tokenize        | sequoia  |\n",
      "| mwt             | sequoia  |\n",
      "| pos             | sequoia  |\n",
      "| pretrain        | conll17  |\n",
      "| backward_charlm | newswiki |\n",
      "| forward_charlm  | newswiki |\n",
      "==============================\n",
      "\n",
      "2023-07-14 21:34:36 INFO: File exists: C:\\Users\\ethan\\stanza_resources\\fr\\tokenize\\sequoia.pt\n",
      "2023-07-14 21:34:36 INFO: File exists: C:\\Users\\ethan\\stanza_resources\\fr\\mwt\\sequoia.pt\n",
      "2023-07-14 21:34:36 INFO: File exists: C:\\Users\\ethan\\stanza_resources\\fr\\pos\\sequoia.pt\n",
      "2023-07-14 21:34:36 INFO: File exists: C:\\Users\\ethan\\stanza_resources\\fr\\pretrain\\conll17.pt\n",
      "2023-07-14 21:34:36 INFO: File exists: C:\\Users\\ethan\\stanza_resources\\fr\\backward_charlm\\newswiki.pt\n",
      "2023-07-14 21:34:36 INFO: File exists: C:\\Users\\ethan\\stanza_resources\\fr\\forward_charlm\\newswiki.pt\n",
      "2023-07-14 21:34:36 INFO: Finished downloading models and saved to C:\\Users\\ethan\\stanza_resources.\n",
      "2023-07-14 21:34:36 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aecb439439e456f936ab78fe71c8dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 21:34:37 INFO: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | sequoia |\n",
      "| mwt       | sequoia |\n",
      "| pos       | sequoia |\n",
      "=======================\n",
      "\n",
      "2023-07-14 21:34:37 INFO: Using device: cpu\n",
      "2023-07-14 21:34:37 INFO: Loading: tokenize\n",
      "2023-07-14 21:34:37 INFO: Loading: mwt\n",
      "2023-07-14 21:34:37 INFO: Loading: pos\n",
      "2023-07-14 21:34:37 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#Tokenise and pos-tag FR sentences first using sequoia-trained treebank model. We don't need anything more.\n",
    "import stanza\n",
    "\n",
    "stanza.download('fr', processors='tokenize, mwt, pos', package='sequoia')\n",
    "nlp_fr = stanza.Pipeline('fr', processors='tokenize, mwt, pos', package='sequoia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e44878e0-b1ff-40cd-a29e-7a584c00e246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [04:34<00:00,  2.14it/s]\n"
     ]
    }
   ],
   "source": [
    "#Extract only the information we need - PoS tags and text.\n",
    "fr_tagged = []\n",
    "for sentence in tqdm(fr_sent):\n",
    "    doc = nlp_fr(sentence)\n",
    "    tokens = [{\"text\" : word.text, \"upos\" : word.upos} for sent in doc.sentences for word in sent.words]\n",
    "    fr_tagged.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4552c56a-0b68-4869-be2d-1fff0009ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This allows us to generate a pandas dataframe. We won't remove HTML tags, because they also appear in the source language set.\n",
    "import pandas as pd\n",
    "term_list = pd.read_csv(\"wmt22_ner_terms.txt\", sep = \"\\t\", header=None, names = [\"sent_ID\", \"term\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "97ac5045-bb3a-4e6d-b447-11bff67416ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#And we simply aggregate counts. We will take note of casing here, but generate a separate list without casing.\n",
    "def find_count_in_sentence_exact_match(row):\n",
    "    query = row[\"term\"]\n",
    "    return len([found for found in fr_tagged[row[\"sent_ID\"]] if query == found[\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c190b81-f020-4131-89ce-3625a9673a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_list[\"count\"] = term_list.apply(find_count_in_sentence_exact_match, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bd99635c-33b0-49f0-bc85-74cfa5ede61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_list = term_list.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "198cf5ec-8203-4df1-9d62-8f6799e09773",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_list.to_csv(\"wmt22_ner_terms_counts.txt\", sep = \"\\t\", header = False, index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a4890-52f6-4d8e-9e3a-1ef5e51b455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we will generate a separate list without casing, and aggregate counts as usual. Interestingly, we have the same number of rows, indicating that there are no terminologies which \n",
    "#appear multiple times within a single sentence, but with different casing. This means that we can stop here for now.\n",
    "#term_list_uncased = term_list[[\"sent_ID\", \"term\"]]\n",
    "#term_list_uncased[\"uncased_term\"] = term_list_uncased[\"term\"].apply(str.lower)\n",
    "#term_list_uncased = term_list_uncased.drop(columns = \"term\")\n",
    "#term_list_uncased = term_list_uncased.drop_duplicates().reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
