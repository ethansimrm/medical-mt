{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b812f08-a1d0-4b0d-9fab-1a6750534171",
   "metadata": {},
   "source": [
    "# Terminology Tagging for Choi et al. (2022)'s Soft Constraint Method\n",
    "\n",
    "We've obtained our noun alignments for 15% of our training set, and we will now determine the intersection of our filtered glossary and these nouns. We will then randomly select up to three noun-noun pairs per sentence, and annotate them with our tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f52e40-c401-4462-b13f-eed26e0db3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "#Converts data in src [TAB] tgt [NEWLINE] format to a format suitable for model training\n",
    "def convertToDictFormat(data):\n",
    "    source = []\n",
    "    target = []\n",
    "    for example in data:\n",
    "        example = example.strip()\n",
    "        sentences = example.split(\"\\t\")\n",
    "        source.append(sentences[0])\n",
    "        target.append(sentences[1])\n",
    "    ready = Dataset.from_dict({\"en\":source, \"fr\":target})\n",
    "    return ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0b4c604-1700-435c-b6a1-bcee9b7a4573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (C:/Users/ethan/.cache/huggingface/datasets/ethansimrm___text/ethansimrm--choi_filtered_cleaned_glossary-55c8ab5554eb133f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    }
   ],
   "source": [
    "#Load in clean glossary and convert to Dataset object\n",
    "glossary_terms = load_dataset(\"ethansimrm/choi_filtered_cleaned_glossary\", split = \"train\")\n",
    "terms_ready = convertToDictFormat(glossary_terms['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a99460ce-fdd5-42f2-b0bc-5a819f6286b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in noun alignments\n",
    "import pandas as pd\n",
    "noun_alignments_df = pd.read_csv(\"noun-alignments.txt\", sep = \"\\t\", header = None, names = [\"sent_ID\", \"en\", \"fr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1fb897d8-9a11-4056-be0e-a09413962707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add new columns which contain lowercase words\n",
    "lower_en = []\n",
    "lower_fr = []\n",
    "for en_noun in noun_alignments_df[\"en\"]:\n",
    "    lower_en.append(en_noun.lower())\n",
    "for fr_noun in noun_alignments_df[\"fr\"]:\n",
    "    lower_fr.append(fr_noun.lower())\n",
    "noun_alignments_df[\"en_lower\"] = lower_en\n",
    "noun_alignments_df[\"fr_lower\"] = lower_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e6d6ad29-d9af-4be8-b1bd-5c0cf2bdb56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We also need to kick out terms which differ only in casing, because multiple occurrences of the same term pair in a sentence may be duplicated otherwise. \n",
    "#We do this by converting to a pandas dataframe, and converting back into a dataset.\n",
    "lower_en_terms = []\n",
    "lower_fr_terms = []\n",
    "for en_term in terms_ready[\"en\"]:\n",
    "    lower_en_terms.append(en_term.lower())\n",
    "for fr_term in terms_ready[\"fr\"]:\n",
    "    lower_fr_terms.append(fr_term.lower())\n",
    "terms_lowercased = Dataset.from_dict({\"en\":lower_en_terms, \"fr\":lower_fr_terms})\n",
    "lowercase_terms_df = pd.DataFrame(terms_lowercased).drop_duplicates(ignore_index = True)\n",
    "lowercase_terms_ready = Dataset.from_pandas(lowercase_terms_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "651abfd1-ed07-4f36-8a17-3637c89ac980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4970/4970 [02:36<00:00, 31.77it/s]\n"
     ]
    }
   ],
   "source": [
    "#Determine intersection the fast way, in a case-independent manner - also captures terminology translations with different casing due to different sentence structure\n",
    "from tqdm import tqdm\n",
    "found = []\n",
    "for term in tqdm(lowercase_terms_ready):\n",
    "    sentences_with_term = noun_alignments_df[(noun_alignments_df[\"en_lower\"] == term[\"en\"].lower()) & (noun_alignments_df[\"fr_lower\"] == term[\"fr\"].lower())]\n",
    "    for row in sentences_with_term.itertuples(index=False):\n",
    "        found.append([row[0], row[1], row[2]]) #However, we will only use the original sentence casing to preserve correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4444b3a8-2557-4822-bfce-f49e35501c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_nouns = pd.DataFrame(found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "85c595d2-5ded-4ccf-8742-151a7428e2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8980</td>\n",
       "      <td>abdomen</td>\n",
       "      <td>abdomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11600</td>\n",
       "      <td>abdomen</td>\n",
       "      <td>abdomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12284</td>\n",
       "      <td>abdomen</td>\n",
       "      <td>Abdomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12687</td>\n",
       "      <td>abdomen</td>\n",
       "      <td>abdomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13272</td>\n",
       "      <td>abdomen</td>\n",
       "      <td>abdomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52941</th>\n",
       "      <td>28067</td>\n",
       "      <td>fever</td>\n",
       "      <td>fievre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52942</th>\n",
       "      <td>31281</td>\n",
       "      <td>fever</td>\n",
       "      <td>Fievre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52943</th>\n",
       "      <td>49679</td>\n",
       "      <td>fever</td>\n",
       "      <td>fievre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52944</th>\n",
       "      <td>13673</td>\n",
       "      <td>PPAR</td>\n",
       "      <td>PPAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52945</th>\n",
       "      <td>80421</td>\n",
       "      <td>TIA</td>\n",
       "      <td>AIT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52946 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0        1        2\n",
       "0       8980  abdomen  abdomen\n",
       "1      11600  abdomen  abdomen\n",
       "2      12284  abdomen  Abdomen\n",
       "3      12687  abdomen  abdomen\n",
       "4      13272  abdomen  abdomen\n",
       "...      ...      ...      ...\n",
       "52941  28067    fever   fievre\n",
       "52942  31281    fever   Fievre\n",
       "52943  49679    fever   fievre\n",
       "52944  13673     PPAR     PPAR\n",
       "52945  80421      TIA      AIT\n",
       "\n",
       "[52946 rows x 3 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c70340b6-69bc-470b-a4ec-d49d9ff19113",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_nouns.columns = ['sent_ID', 'en', 'fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b61af4d3-0af8-4dd3-aae1-91b5f24d52d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40034"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(found_nouns[\"sent_ID\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "688b362c-dfed-474b-b387-4f285cc8b497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40034/40034 [00:22<00:00, 1766.49it/s]\n"
     ]
    }
   ],
   "source": [
    "#Perform per-sentence sampling\n",
    "from tqdm import tqdm\n",
    "final_annotation_candidates = []\n",
    "for sentence_ID in tqdm(list(found_nouns[\"sent_ID\"].unique())):\n",
    "    nouns_to_annotate = found_nouns[found_nouns[\"sent_ID\"] == sentence_ID]\n",
    "    if (len(nouns_to_annotate) > 3):\n",
    "        nouns_to_annotate = nouns_to_annotate.sample(n=3, random_state = 42) #Choose a maximum of three nouns per sentence to annotate, seed = 42 for reproducibility\n",
    "    for row in nouns_to_annotate.itertuples(index=False):\n",
    "        final_annotation_candidates.append([row[0], row[1], row[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "97d614a2-460a-4bb0-ad51-cb62d76c7a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_nouns = pd.DataFrame(final_annotation_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5bc29378-c460-4867-a794-a5df95b05a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_nouns.columns = ['sent_ID', 'en', 'fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b7422f87-b366-45dd-9ace-d234eaa6c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_nouns_sorted = selected_nouns.sort_values(by = \"sent_ID\", ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0b0cee8a-6153-440e-bca2-6f4f0a443004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save for record-keeping purposes - these are all the nouns we will annotate, along with the sentences they are in\n",
    "selected_nouns_sorted.to_csv('selected_nouns.txt', sep=\"\\t\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a9f51990-cbe7-42df-b77a-1c97ee2fee75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (C:/Users/ethan/.cache/huggingface/datasets/ethansimrm___text/ethansimrm--choi_sampled_train-180f43e915e77b7c/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    }
   ],
   "source": [
    "#Load in training data\n",
    "sampled_train = load_dataset(\"ethansimrm/choi_sampled_train\", split = \"train\")\n",
    "sampled_train_ready = convertToDictFormat(sampled_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7f855882-d04f-4dfc-8f85-da3d76161339",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_train_to_modify = sampled_train_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fc074960-8abf-4b5e-af4b-a54fa12782ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97378it [04:05, 396.90it/s]\n"
     ]
    }
   ],
   "source": [
    "#Add tokens to source side of training data. We will have to convert to our format, then convert back to a Dataset to do this - we cannot access sentence indices using .map().\n",
    "source_sentences = []\n",
    "target_sentences = []\n",
    "for sent_num, bitext in tqdm(enumerate(sampled_train_to_modify)):\n",
    "    nouns_in_sentence = selected_nouns_sorted[selected_nouns_sorted[\"sent_ID\"] == sent_num]\n",
    "    #Account for nested replacements such as <term_start> <term_start> EN <term_end> FR <term_trans> <term_end> FR <term_trans> using duplicate counts\n",
    "    nouns_in_sentence = nouns_in_sentence.groupby(nouns_in_sentence.columns.tolist(),as_index=False).size() #Adds additional size column and removes duplicates\n",
    "    for row in nouns_in_sentence.itertuples(index=False):\n",
    "        if (bitext['en'].find(\"<term_start> \" + row[1] + \" <term_end>\") != -1): \n",
    "            #Indicates differences in target casing, leading to nested replacements; ignore as we did not preserve positional information during noun alignment (only 78/52437 occurrences affected).\n",
    "            continue\n",
    "        #Assumed that duplicate terminology tags per sentence are not ignored due to random nature of sampling\n",
    "        bitext['en'] = bitext['en'].replace(row[1], \"<term_start> \" + row[1] + \" <term_end> \" + row[2] + \" <term_trans>\", row[3]) \n",
    "    source_sentences.append(bitext['en'])\n",
    "    target_sentences.append(bitext['fr'])\n",
    "sampled_train_annotated = Dataset.from_dict({\"en\":source_sentences, \"fr\":target_sentences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2774395a-875e-4d44-ab37-9268759cf2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ready for upload\n",
    "output = open(\"choi_annotated_sampled_train.txt\", \"w\", encoding = \"utf8\")\n",
    "for bitext in sampled_train_annotated:\n",
    "    output.write(bitext[\"en\"] + \"\\t\" + bitext[\"fr\"] + \"\\n\")\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4663f82e-26d6-47b1-9674-9e567caf2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are some minor anomalies like nested terms (e.g., dysplasia and chondrodysplasia) - going through it manually as we only have 1-2 occurrences per case.\n",
    "#In all, we have 52359 annotated terms.\n",
    "'''\n",
    "Sentence 34351:\n",
    "\n",
    "Original\n",
    "<term_start> Chromatin <term_end> <term_start> chromatin <term_end> chromatine <term_trans>e <term_trans> structure. I. Physico-chemical study of chromatin stability.\t\n",
    "Contribution à l'étude de la structure de la chromatine. I. Etude physico-chimique de la stabilité de la chromatine.\n",
    "\n",
    "Modified\n",
    "<term_start> Chromatin <term_end> chromatine <term_trans> structure. I. Physico-chemical study of <term_start> chromatin <term_end> chromatine <term_trans> stability.\t\n",
    "Contribution à l'étude de la structure de la chromatine. I. Etude physico-chimique de la stabilité de la chromatine.\n",
    "\n",
    "Sentence 51973:\n",
    "\n",
    "Original\n",
    "A <term_start> case <term_end> cas <term_trans> of <term_start> chondro<term_start> dysplasia <term_end> dysplasie <term_trans> <term_end> chondrodysplasie <term_trans> \n",
    "difficult to classify: metatropic dwarfism or Kozlowski type spondylometaphyseal dysplasia?\t\n",
    "Un cas de chondrodysplasie difficilement classable: nanisme métatropique ou dysplasie spondylo-métaphysaire type Kozlowski?\n",
    "\n",
    "Modified\n",
    "A <term_start> case <term_end> cas <term_trans> of <term_start> chondrodysplasia <term_end> chondrodysplasie <term_trans> difficult to classify: \n",
    "metatropic dwarfism or Kozlowski type spondylometaphyseal <term_start> dysplasia <term_end> dysplasie <term_trans>?\t\n",
    "Un cas de chondrodysplasie difficilement classable: nanisme métatropique ou dysplasie spondylo-métaphysaire type Kozlowski?\n",
    "\n",
    "Sentence 77857:\n",
    "\n",
    "Original\n",
    "<term_start> Hypertension <term_end> <term_start> hypertension <term_end> hypertension <term_trans> <term_trans> is of particular interest, \n",
    "because components of the renin-angiotensin system (RAS), which are critically involved in the pathophysiology of hypertension, are also implicated in COVID-19.\t\n",
    "L'hypertension suscite un intérêt particulier, car certaines composantes du système rénine-angiotensine (SRA), dont le rôle est crucial dans la physiopathologie de l'hypertension, \n",
    "sont également en cause dans la COVID-19.\n",
    "\n",
    "Modified\n",
    "<term_start> Hypertension <term_end> hypertension <term_trans> is of particular interest, because components of the renin-angiotensin system (RAS), \n",
    "which are critically involved in the pathophysiology of <term_start> hypertension <term_end> hypertension <term_trans>, are also implicated in COVID-19.\t\n",
    "L'hypertension suscite un intérêt particulier, car certaines composantes du système rénine-angiotensine (SRA), dont le rôle est crucial dans la physiopathologie de l'hypertension, \n",
    "sont également en cause dans la COVID-19.\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
