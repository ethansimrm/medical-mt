{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers[sentencepiece]==4.28.0 datasets sacrebleu evaluate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-26T10:02:07.428391Z","iopub.execute_input":"2023-05-26T10:02:07.428809Z","iopub.status.idle":"2023-05-26T10:02:17.343537Z","shell.execute_reply.started":"2023-05-26T10:02:07.428778Z","shell.execute_reply":"2023-05-26T10:02:17.342580Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: transformers[sentencepiece]==4.28.0 in /opt/conda/lib/python3.10/site-packages (4.28.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: sacrebleu in /opt/conda/lib/python3.10/site-packages (2.3.1)\nRequirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (3.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (0.14.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (4.64.1)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (0.1.99)\nRequirement already satisfied: protobuf<=3.20.2 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (3.20.2)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (9.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2.7.0)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers[sentencepiece]==4.28.0) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[sentencepiece]==4.28.0) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.28.0) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.28.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.28.0) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value = user_secrets.get_secret(\"HF_KEY\")\nlogin(secret_value)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:02:17.345451Z","iopub.execute_input":"2023-05-26T10:02:17.345784Z","iopub.status.idle":"2023-05-26T10:02:17.679874Z","shell.execute_reply.started":"2023-05-26T10:02:17.345756Z","shell.execute_reply":"2023-05-26T10:02:17.678986Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\n\nprint(transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:02:17.681173Z","iopub.execute_input":"2023-05-26T10:02:17.682401Z","iopub.status.idle":"2023-05-26T10:02:17.689762Z","shell.execute_reply.started":"2023-05-26T10:02:17.682354Z","shell.execute_reply":"2023-05-26T10:02:17.687416Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"4.29.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\n\n#wmt16_train = load_dataset(\"ethansimrm/wmt16_biomed\", use_auth_token=True) #We won't need to use this for now\nwmt16_test = load_dataset(\"ethansimrm/wmt16_biomed_test\", use_auth_token=True)\nwmt16_gold = load_dataset(\"ethansimrm/wmt16_biomed_gold\", use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:02:17.692565Z","iopub.execute_input":"2023-05-26T10:02:17.694199Z","iopub.status.idle":"2023-05-26T10:02:19.509399Z","shell.execute_reply.started":"2023-05-26T10:02:17.694143Z","shell.execute_reply":"2023-05-26T10:02:19.508157Z"},"trusted":true},"execution_count":94,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"859dbfa9cf6941afa6576c21c662b8c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60b5c57ed64544f8be1f736593cdcc53"}},"metadata":{}}]},{"cell_type":"markdown","source":"Let's aim low for now. Given a (small) pre-trained large language model - Marian or T5-Small, etc, we wish to evaluate baseline performance on the WMT16 Biomedical Test Set. \n- We have converted the test set and the gold-standard answers to CSV files and uploaded both to the HuggingFace Hub.\n- Intuitively, we must extract source sentences from the test set and target sentences from the gold set, but only where there is a direct correspondence between them.\n- We know for sure that \"passage/0\" (the scientific paper titles) - are one sentence long in both languages, so there is a direct correspondence - we cannot guarantee the same for the rest of the sentences.\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-26T09:31:25.313684Z","iopub.execute_input":"2023-05-26T09:31:25.314110Z","iopub.status.idle":"2023-05-26T09:31:25.326116Z","shell.execute_reply.started":"2023-05-26T09:31:25.314075Z","shell.execute_reply":"2023-05-26T09:31:25.324353Z"}}},{"cell_type":"markdown","source":"Additional Information:\n- We could simply truncate the gold-standard answers to the length of the source sentences, but we only require a quick-and-dirty evaluation today.\n- Furthermore, the test set contains French abstracts which contain information which cannot be inferred from the English source; we cannot assume a 1-1 faithful translation.","metadata":{}},{"cell_type":"code","source":"#The relevant columns for the abstract titles are:\nsource_sentences = wmt16_test[\"test\"][\"passage/0/sentence/0/text\"]\ntarget_sentences = wmt16_gold[\"train\"][\"passage/0/sentence/0/text\"]","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:02:19.510993Z","iopub.execute_input":"2023-05-26T10:02:19.511358Z","iopub.status.idle":"2023-05-26T10:02:19.520556Z","shell.execute_reply.started":"2023-05-26T10:02:19.511332Z","shell.execute_reply":"2023-05-26T10:02:19.519166Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"from datasets import load_metric\nmetric = load_metric(\"sacrebleu\")","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:02:19.522655Z","iopub.execute_input":"2023-05-26T10:02:19.522992Z","iopub.status.idle":"2023-05-26T10:02:20.138742Z","shell.execute_reply.started":"2023-05-26T10:02:19.522963Z","shell.execute_reply":"2023-05-26T10:02:20.136922Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"#This used a pretrained model from an earlier tutorial - it's quite bad. We need to train our base models\n#on at least some out of domain EN-FR data, and upload them to the Hub before evaluating on the test set.\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5Model, pipeline\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"ethansimrm/test_t5_small_example_kaggle3\") \ntranslator = pipeline(\"translation\", model=model, tokenizer=tokenizer, use_auth_token = True)\n#for s in source_sentences:\n    #source = \"translate English to French: \" + s\n    #if(source[len(source) - 1] == '.'):\n        #source = source[:-1]\n    #print(source)\n    #result = translator(source)\n    #print(result)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:02:20.140907Z","iopub.execute_input":"2023-05-26T10:02:20.141362Z","iopub.status.idle":"2023-05-26T10:02:21.498062Z","shell.execute_reply.started":"2023-05-26T10:02:20.141324Z","shell.execute_reply":"2023-05-26T10:02:21.496971Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"Let's choose a smaller (ish?) but still somewhat relevant OOD corpus - the UN Parallel Corpus, which is often incorporated in training data for WMT tasks.","metadata":{}},{"cell_type":"code","source":"training_data = load_dataset(\"un_pc\", \"en-fr\") ","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:26:20.357649Z","iopub.execute_input":"2023-05-26T10:26:20.358055Z","iopub.status.idle":"2023-05-26T10:46:52.330960Z","shell.execute_reply.started":"2023-05-26T10:26:20.358027Z","shell.execute_reply":"2023-05-26T10:46:52.329635Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset un_pc/en-fr (download: 2.47 GiB, generated: 9.05 GiB, post-processed: Unknown size, total: 11.53 GiB) to /root/.cache/huggingface/datasets/un_pc/en-fr/1.0.0/1360070a820db42f7427f5a98416dd3a1c956ae069b994bf2ec0b83ae16dcaee...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/30340652 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset un_pc downloaded and prepared to /root/.cache/huggingface/datasets/un_pc/en-fr/1.0.0/1360070a820db42f7427f5a98416dd3a1c956ae069b994bf2ec0b83ae16dcaee. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7eeb95552d347a7ad0d7942a9afdec0"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\ncheckpoint = \"t5-small\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"source_lang = \"en\"\ntarget_lang = \"fr\"\nprefix = \"translate English to French: \"\ndef preprocess_function(examples):\n    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]] \n    targets = [example[target_lang] for example in examples[\"translation\"]]\n    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n    return model_inputs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}