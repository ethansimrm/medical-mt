{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers[sentencepiece]==4.28.0 datasets sacrebleu evaluate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-26T10:02:07.428391Z","iopub.execute_input":"2023-05-26T10:02:07.428809Z","iopub.status.idle":"2023-05-26T10:02:17.343537Z","shell.execute_reply.started":"2023-05-26T10:02:07.428778Z","shell.execute_reply":"2023-05-26T10:02:17.342580Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: transformers[sentencepiece]==4.28.0 in /opt/conda/lib/python3.10/site-packages (4.28.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: sacrebleu in /opt/conda/lib/python3.10/site-packages (2.3.1)\nRequirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (3.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (0.14.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (4.64.1)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (0.1.99)\nRequirement already satisfied: protobuf<=3.20.2 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]==4.28.0) (3.20.2)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (9.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2.7.0)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers[sentencepiece]==4.28.0) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[sentencepiece]==4.28.0) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.28.0) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.28.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.28.0) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value = user_secrets.get_secret(\"HF_KEY\")\nlogin(secret_value)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:02:17.345451Z","iopub.execute_input":"2023-05-26T10:02:17.345784Z","iopub.status.idle":"2023-05-26T10:02:17.679874Z","shell.execute_reply.started":"2023-05-26T10:02:17.345756Z","shell.execute_reply":"2023-05-26T10:02:17.678986Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\n\nprint(transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:02:17.681173Z","iopub.execute_input":"2023-05-26T10:02:17.682401Z","iopub.status.idle":"2023-05-26T10:02:17.689762Z","shell.execute_reply.started":"2023-05-26T10:02:17.682354Z","shell.execute_reply":"2023-05-26T10:02:17.687416Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"4.29.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\n\n#wmt16_train = load_dataset(\"ethansimrm/wmt16_biomed\", use_auth_token=True) #We won't need to use this for now\nwmt16_test = load_dataset(\"ethansimrm/wmt16_biomed_test\", use_auth_token=True)\nwmt16_gold = load_dataset(\"ethansimrm/wmt16_biomed_gold\", use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:02:17.692565Z","iopub.execute_input":"2023-05-26T10:02:17.694199Z","iopub.status.idle":"2023-05-26T10:02:19.509399Z","shell.execute_reply.started":"2023-05-26T10:02:17.694143Z","shell.execute_reply":"2023-05-26T10:02:19.508157Z"},"trusted":true},"execution_count":94,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"859dbfa9cf6941afa6576c21c662b8c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60b5c57ed64544f8be1f736593cdcc53"}},"metadata":{}}]},{"cell_type":"markdown","source":"Let's aim low for now. Given a (small) pre-trained large language model - Marian or T5-Small, etc, we wish to evaluate baseline performance on the WMT16 Biomedical Test Set. \n- We have converted the test set and the gold-standard answers to CSV files and uploaded both to the HuggingFace Hub.\n- Intuitively, we must extract source sentences from the test set and target sentences from the gold set, but only where there is a direct correspondence between them.\n- We know for sure that \"passage/0\" (the scientific paper titles) - are one sentence long in both languages, so there is a direct correspondence - we cannot guarantee the same for the rest of the sentences.\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-26T09:31:25.313684Z","iopub.execute_input":"2023-05-26T09:31:25.314110Z","iopub.status.idle":"2023-05-26T09:31:25.326116Z","shell.execute_reply.started":"2023-05-26T09:31:25.314075Z","shell.execute_reply":"2023-05-26T09:31:25.324353Z"}}},{"cell_type":"markdown","source":"Additional Information:\n- We could simply truncate the gold-standard answers to the length of the source sentences, but we only require a quick-and-dirty evaluation today.\n- Furthermore, the test set contains French abstracts which contain information which cannot be inferred from the English source; we cannot assume a 1-1 faithful translation.","metadata":{}},{"cell_type":"code","source":"#The relevant columns for the abstract titles are:\nsource_sentences = wmt16_test[\"test\"][\"passage/0/sentence/0/text\"]\ntarget_sentences = wmt16_gold[\"train\"][\"passage/0/sentence/0/text\"]","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:02:19.510993Z","iopub.execute_input":"2023-05-26T10:02:19.511358Z","iopub.status.idle":"2023-05-26T10:02:19.520556Z","shell.execute_reply.started":"2023-05-26T10:02:19.511332Z","shell.execute_reply":"2023-05-26T10:02:19.519166Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"from datasets import load_metric\nmetric = load_metric(\"sacrebleu\")","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:02:19.522655Z","iopub.execute_input":"2023-05-26T10:02:19.522992Z","iopub.status.idle":"2023-05-26T10:02:20.138742Z","shell.execute_reply.started":"2023-05-26T10:02:19.522963Z","shell.execute_reply":"2023-05-26T10:02:20.136922Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"#This used a pretrained model from an earlier tutorial - it's quite bad. We need to train our base models\n#on at least some out of domain EN-FR data, and upload them to the Hub before evaluating on the test set.\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5Model, pipeline\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"ethansimrm/test_t5_small_example_kaggle3\") \ntranslator = pipeline(\"translation\", model=model, tokenizer=tokenizer, use_auth_token = True)\n#for s in source_sentences:\n    #source = \"translate English to French: \" + s\n    #if(source[len(source) - 1] == '.'):\n        #source = source[:-1]\n    #print(source)\n    #result = translator(source)\n    #print(result)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:02:20.140907Z","iopub.execute_input":"2023-05-26T10:02:20.141362Z","iopub.status.idle":"2023-05-26T10:02:21.498062Z","shell.execute_reply.started":"2023-05-26T10:02:20.141324Z","shell.execute_reply":"2023-05-26T10:02:21.496971Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"The usual corpora used in research are massive. Let's get a (much) smaller one out - the news commentary corpus from HuggingFace (OPUS; converted into sentence alignments) should work.\nWe could have obtained EMEA (and many more) corpora from OPUS, but we don't have the time to convert this into HuggingFace format right now.","metadata":{}},{"cell_type":"code","source":"training_data = load_dataset(\"news_commentary\", \"en-fr\") ","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:53:50.620467Z","iopub.execute_input":"2023-05-26T10:53:50.621114Z","iopub.status.idle":"2023-05-26T10:54:03.987960Z","shell.execute_reply.started":"2023-05-26T10:53:50.621083Z","shell.execute_reply":"2023-05-26T10:54:03.986565Z"},"trusted":true},"execution_count":119,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfe7446b98904afc94d2448601cad1bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/7.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a21ee51b1ef245a88e3b5fcba62c8c02"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset news_commentary/en-fr (download: 23.83 MiB, generated: 67.08 MiB, post-processed: Unknown size, total: 90.91 MiB) to /root/.cache/huggingface/datasets/news_commentary/en-fr/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/25.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fab1c3f226c34003a9a913850b770b3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/209479 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset news_commentary downloaded and prepared to /root/.cache/huggingface/datasets/news_commentary/en-fr/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce51870c719041be8bab96a9c15ef395"}},"metadata":{}}]},{"cell_type":"code","source":"training_data = training_data['train'].train_test_split(test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:54:55.521197Z","iopub.execute_input":"2023-05-26T10:54:55.521667Z","iopub.status.idle":"2023-05-26T10:54:55.608022Z","shell.execute_reply.started":"2023-05-26T10:54:55.521634Z","shell.execute_reply":"2023-05-26T10:54:55.605998Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"training_data['train'][0]","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:55:07.818873Z","iopub.execute_input":"2023-05-26T10:55:07.819262Z","iopub.status.idle":"2023-05-26T10:55:07.832172Z","shell.execute_reply.started":"2023-05-26T10:55:07.819235Z","shell.execute_reply":"2023-05-26T10:55:07.831325Z"},"trusted":true},"execution_count":123,"outputs":[{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"{'id': '128044',\n 'translation': {'en': 'Europe’s major competitors are turning climate change into an opportunity to encourage growth and create high-quality jobs in rapidly innovating economic sectors.',\n  'fr': 'Les principaux concurrents de l’Europe font du changement climatique une occasion pour&#160; encourager la croissance et créer des emplois de haut niveau dans des secteurs économiques en pleine mutation.'}}"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\ncheckpoint = \"t5-small\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:58:09.838780Z","iopub.execute_input":"2023-05-26T10:58:09.839216Z","iopub.status.idle":"2023-05-26T10:58:10.115524Z","shell.execute_reply.started":"2023-05-26T10:58:09.839191Z","shell.execute_reply":"2023-05-26T10:58:10.114745Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"source_lang = \"en\"\ntarget_lang = \"fr\"\nprefix = \"translate English to French: \"\ndef preprocess_function(examples):\n    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]] \n    targets = [example[target_lang] for example in examples[\"translation\"]]\n    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:58:13.862260Z","iopub.execute_input":"2023-05-26T10:58:13.862701Z","iopub.status.idle":"2023-05-26T10:58:13.869242Z","shell.execute_reply.started":"2023-05-26T10:58:13.862676Z","shell.execute_reply":"2023-05-26T10:58:13.867884Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"tokenized_train = training_data.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:58:34.751220Z","iopub.execute_input":"2023-05-26T10:58:34.752672Z","iopub.status.idle":"2023-05-26T10:59:14.670106Z","shell.execute_reply.started":"2023-05-26T10:58:34.752605Z","shell.execute_reply":"2023-05-26T10:59:14.668365Z"},"trusted":true},"execution_count":126,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/168 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5136e5dddbe4bc7915fe8ee97d3cc0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/42 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ced262b883ca464bab5a2e54e781a32f"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T10:59:57.114306Z","iopub.execute_input":"2023-05-26T10:59:57.114741Z","iopub.status.idle":"2023-05-26T10:59:57.120418Z","shell.execute_reply.started":"2023-05-26T10:59:57.114713Z","shell.execute_reply":"2023-05-26T10:59:57.118982Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef postprocess_text(preds, labels): \n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    return preds, labels\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True) #Convert back into words\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id) #Ignore padded labels added by the data collator to the test set\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True) \n\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels) #Remove leading and trailing spaces\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels) #BLEU score for provided input and references\n    result = {\"bleu\": result[\"score\"]}\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens) #Compute mean prediction length\n    result = {k: round(v, 4) for k, v in result.items()} #Round score to 4dp\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-05-26T11:00:18.649014Z","iopub.execute_input":"2023-05-26T11:00:18.649536Z","iopub.status.idle":"2023-05-26T11:00:18.660781Z","shell.execute_reply.started":"2023-05-26T11:00:18.649470Z","shell.execute_reply":"2023-05-26T11:00:18.658609Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T11:00:40.584796Z","iopub.execute_input":"2023-05-26T11:00:40.585218Z","iopub.status.idle":"2023-05-26T11:00:43.869269Z","shell.execute_reply.started":"2023-05-26T11:00:40.585189Z","shell.execute_reply":"2023-05-26T11:00:43.868117Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"import torch\n\ntraining_args = Seq2SeqTrainingArguments( #Collects hyperparameters\n    output_dir=\"t5_small_prelim_news\",\n    evaluation_strategy=\"epoch\", #Evaluates at the end of each epoch\n    learning_rate=2e-5, #Initial learning rate for AdamW\n    per_device_train_batch_size=16, #Minibatch learning\n    per_device_eval_batch_size=16, #Batch size for evaluation\n    weight_decay=0.01, #Weight decay for loss computation; Loss = Loss + WD * sum (weights squared)\n    save_total_limit=3, #Number of checkpoints to save\n    num_train_epochs=2,\n    predict_with_generate=True, #Use with ROUGE/BLEU and other translation metrics (see below)\n    fp16=True, #Remove fp16 = True if not using CUDA\n    push_to_hub=True,\n)\n\ntrainer = Seq2SeqTrainer( #Saves us from writing our own training loops\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_books[\"train\"],\n    eval_dataset=tokenized_booksTest[\"train\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T11:01:27.374653Z","iopub.execute_input":"2023-05-26T11:01:27.375092Z","iopub.status.idle":"2023-05-26T11:01:27.613870Z","shell.execute_reply.started":"2023-05-26T11:01:27.375057Z","shell.execute_reply":"2023-05-26T11:01:27.612419Z"},"trusted":true},"execution_count":130,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[130], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#Collects hyperparameters\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mt5_small_prelim_news\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#Evaluates at the end of each epoch\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#Initial learning rate for AdamW\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#Minibatch learning\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#Batch size for evaluation\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#Weight decay for loss computation; Loss = Loss + WD * sum (weights squared)\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#Number of checkpoints to save\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_with_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#Use with ROUGE/BLEU and other translation metrics (see below)\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#Remove fp16 = True if not using CUDA\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer( \u001b[38;5;66;03m#Saves us from writing our own training loops\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     19\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     25\u001b[0m )\n","File \u001b[0;32m<string>:115\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, xpu_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, sortish_sampler, predict_with_generate, generation_max_length, generation_num_beams, generation_config)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1263\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1262\u001b[0m ):\n\u001b[0;32m-> 1263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1266\u001b[0m     )\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1276\u001b[0m ):\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--bf16_full_eval`) can only be used on CUDA or CPU/TPU/NeuronCore devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1280\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA devices."],"ename":"ValueError","evalue":"FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA devices.","output_type":"error"}]},{"cell_type":"code","source":"trainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{},"execution_count":null,"outputs":[]}]}