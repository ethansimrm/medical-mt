{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab83478-7ab5-4d12-b236-7f755d65cd68",
   "metadata": {},
   "source": [
    "# TextRank Testing\n",
    "\n",
    "Here, we will explore the Textrank algorithm implemented in pytextrank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb6a8b9-5b78-4730-b002-df6967e3321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "#Converts data in src [TAB] tgt [NEWLINE] format to a format suitable for model training\n",
    "def convertToDictFormat(data):\n",
    "    source = []\n",
    "    target = []\n",
    "    for example in data:\n",
    "        example = example.strip()\n",
    "        sentences = example.split(\"\\t\")\n",
    "        source.append(sentences[0])\n",
    "        target.append(sentences[1])\n",
    "    ready = Dataset.from_dict({\"en\":source, \"fr\":target})\n",
    "    return ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "807c0304-8df8-45cf-b214-055ced1b97cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (C:/Users/ethan/.cache/huggingface/datasets/ethansimrm___text/ethansimrm--wmt_16_19_22_biomed_train_processed-8662b34233d7661e/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    }
   ],
   "source": [
    "train_data = load_dataset(\"ethansimrm/wmt_16_19_22_biomed_train_processed\", split = \"train\")\n",
    "train_data_ready = convertToDictFormat(train_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "521f6c07-d4db-4146-843e-cc5ef2722b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampled = train_data_ready.train_test_split(train_size = 0.1, seed = 42)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0854e220-e590-4149-9cd0-fef8c94760bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 25016.72it/s]\n"
     ]
    }
   ],
   "source": [
    "#Sample 10% of corpus sentences due to space/efficiency issues - building a graph is very expensive. We will take a random sample a la Ailem for this.\n",
    "from tqdm import tqdm\n",
    "corpus = \"\"\n",
    "for sent in tqdm(train_sampled['fr'][:50]):\n",
    "    corpus += sent + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7a868cef-1190-410f-8287-d4269bf435ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop #We will feed this into the stop words recogniser and the scrubber function\n",
    "fr_stop_words = list(fr_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d7d3703-44a7-412c-8d0e-525c53b37fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = spacy.load(\"fr_dep_news_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8ddbe525-a205-433c-be40-e758e64a6080",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {}\n",
    "for word in fr_stop_words:\n",
    "    doc = sw(word)\n",
    "    for tok in doc:\n",
    "        stop_words[tok.text] = [tok.pos_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3cc29ff2-cae8-4414-900b-c817b6d9061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "@spacy.registry.misc(\"prefix_scrubber\")\n",
    "def prefix_scrubber():\n",
    "    def scrubber_func(span: Span) -> str:\n",
    "        for token in span:\n",
    "            if token.pos_ not in [\"DET\", \"PRON\"]:\n",
    "                break\n",
    "            span = span[1:]\n",
    "        return span.text\n",
    "    return scrubber_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a510fb15-7d42-4aba-b083-640aa8d0ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_dep_news_trf\") #We will then run through the sampled corpus using textrank.\n",
    "nlp.add_pipe(\"textrank\", config={\"stopwords\" : stop_words, \"scrubber\": {\"@misc\": \"prefix_scrubber\"}})\n",
    "nlp.max_length = 1000000 #7m chars for 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "db741e57-0d49-477a-8af2-640bdd6d1cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(corpus, disable = [\"ner\"]) #Kick out NER to make it run faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9196732b-9443-49ee-9938-4e9a0dd0a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open(\"textrank_phrases_and_scores.txt\", \"w\", encoding = \"utf-8\")\n",
    "for phrase in doc._.phrases:\n",
    "    output.write(str(phrase.text) + \"\\t\" + str(phrase.rank) + \"\\n\")\n",
    "output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
