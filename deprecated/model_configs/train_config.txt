learning_rate: 2e-05
train_batch_size: 32
eval_batch_size: 32
seed: 42 [DEFAULT]
optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 [DEFAULT]
lr_scheduler_type: linear [DEFAULT]
num_epochs: 16
mixed_precision_training: Native AMP [DEFAULT]
weight_decay: 0.01
checkpoints_to_save: 1
early stopping patience: 5
early stopping threshold: 0.1 BLEU
eval_steps: 4000
save_steps: 4000