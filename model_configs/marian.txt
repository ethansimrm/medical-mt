learning_rate: 2e-05
train_batch_size: 16
eval_batch_size: 16
seed: 42
optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
lr_scheduler_type: linear
num_epochs: 2
mixed_precision_training: Native AMP
weight_decay: 0.01
checkpoints_to_save: 3
BLEU: Not computed as eval took more than 1st epoch and hardcap of 12h

learning_rate: 2e-05
train_batch_size: 32 #Twice as fast to train, but less accurate
eval_batch_size: 64 #From looking at others' models
seed: 42
optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
lr_scheduler_type: linear
num_epochs: 3
mixed_precision_training: Native AMP
weight_decay: 0.01
checkpoints_to_save: 3
BLEU: