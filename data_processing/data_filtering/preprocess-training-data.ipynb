{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers==4.28.0 sentencepiece sacremoses datasets ","metadata":{"execution":{"iopub.status.busy":"2023-06-18T18:26:04.658624Z","iopub.execute_input":"2023-06-18T18:26:04.658972Z","iopub.status.idle":"2023-06-18T18:26:26.775687Z","shell.execute_reply.started":"2023-06-18T18:26:04.658946Z","shell.execute_reply":"2023-06-18T18:26:26.774917Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting transformers==4.28.0\n  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\nCollecting sacremoses\n  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (3.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (0.14.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.0) (4.64.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from sacremoses) (1.16.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from sacremoses) (8.1.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from sacremoses) (1.2.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (9.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.28.0) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.0) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.0) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nBuilding wheels for collected packages: sacremoses\n  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=800796a2a63a83c4ca992d35d269c3f64b73a4d44edba72c7f97f369b4f4b6f4\n  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\nSuccessfully built sacremoses\nInstalling collected packages: sacremoses, transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.29.2\n    Uninstalling transformers-4.29.2:\n      Successfully uninstalled transformers-4.29.2\nSuccessfully installed sacremoses-0.0.53 transformers-4.28.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\nuser_secrets = UserSecretsClient()\nsecret_value = user_secrets.get_secret(\"HF_KEY\")\nlogin(secret_value)\nsecret_value1 = user_secrets.get_secret(\"WANDB_KEY\")\nwandb.login(key=secret_value1)","metadata":{"execution":{"iopub.status.busy":"2023-06-18T18:26:31.970775Z","iopub.execute_input":"2023-06-18T18:26:31.971232Z","iopub.status.idle":"2023-06-18T18:26:35.328132Z","shell.execute_reply.started":"2023-06-18T18:26:31.971197Z","shell.execute_reply":"2023-06-18T18:26:35.326568Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\ntraining_data = load_dataset(\"ethansimrm/wmt_16_19_22_biomed_train\", split = \"train\", use_auth_token=True) ","metadata":{"execution":{"iopub.status.busy":"2023-06-18T18:26:39.279023Z","iopub.execute_input":"2023-06-18T18:26:39.279707Z","iopub.status.idle":"2023-06-18T18:26:45.090101Z","shell.execute_reply.started":"2023-06-18T18:26:39.279673Z","shell.execute_reply":"2023-06-18T18:26:45.088624Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset text/ethansimrm--wmt_16_19_22_biomed_train to /root/.cache/huggingface/datasets/text/ethansimrm--wmt_16_19_22_biomed_train-3dd4d6328c3dbab1/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5eddfd2c4a742a29cdeb05e8a5d839d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/48.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77f3d88096ab410782fe41d1bf4d532a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45e83a02fcad4bbaa942d89478c1a981"}},"metadata":{}},{"name":"stdout","text":"Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/ethansimrm--wmt_16_19_22_biomed_train-3dd4d6328c3dbab1/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8. Subsequent calls will reuse this data.\n","output_type":"stream"}]},{"cell_type":"code","source":"#len(training_data) #741300 sentence pairs prior to pre-processing\n#training_data['text'][1:10] #Adopting the SOURCE \\t TARGET \\n format is convenient.","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:15:57.676315Z","iopub.execute_input":"2023-06-18T15:15:57.677035Z","iopub.status.idle":"2023-06-18T15:15:57.684004Z","shell.execute_reply.started":"2023-06-18T15:15:57.676999Z","shell.execute_reply":"2023-06-18T15:15:57.682683Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"741300"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import Dataset\nsource = []\ntarget = []\nfor example in training_data['text']:\n    example = example.strip()\n    sentences = example.split(\"\\t\")\n    source.append(sentences[0])\n    target.append(sentences[1])\nready_data = Dataset.from_dict({\"en\":source, \"fr\":target})\n#This dataset can now be used for training or validation per early experiments","metadata":{"execution":{"iopub.status.busy":"2023-06-18T18:26:49.186133Z","iopub.execute_input":"2023-06-18T18:26:49.186745Z","iopub.status.idle":"2023-06-18T18:26:51.250760Z","shell.execute_reply.started":"2023-06-18T18:26:49.186715Z","shell.execute_reply":"2023-06-18T18:26:51.249175Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Seems alright.\nready_data[0]","metadata":{"execution":{"iopub.status.busy":"2023-06-18T18:26:55.033712Z","iopub.execute_input":"2023-06-18T18:26:55.034154Z","iopub.status.idle":"2023-06-18T18:26:55.041143Z","shell.execute_reply.started":"2023-06-18T18:26:55.034123Z","shell.execute_reply":"2023-06-18T18:26:55.039564Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'en': 'Global Health: Where Do Physiotherapy and Rehabilitation Research Fit?',\n 'fr': 'La place des cheveux et des poils dans les rituels et le sacré.'}"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n#Let's start preprocessing according to Wu et al.(2022).\n#First, remove duplicate sentences. \n#We can't do this in datasets, so we must convert it to a pandas DataFrame.\ntempDataset = pd.DataFrame(ready_data)","metadata":{"execution":{"iopub.status.busy":"2023-06-18T18:26:57.259562Z","iopub.execute_input":"2023-06-18T18:26:57.259913Z","iopub.status.idle":"2023-06-18T18:27:24.641180Z","shell.execute_reply.started":"2023-06-18T18:26:57.259886Z","shell.execute_reply":"2023-06-18T18:27:24.639595Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#tempDataset[tempDataset.duplicated()] \n#Wow, there are a lot of duplicate rows, 39540 in total.","metadata":{"execution":{"iopub.status.busy":"2023-06-18T16:01:25.685022Z","iopub.execute_input":"2023-06-18T16:01:25.685439Z","iopub.status.idle":"2023-06-18T16:01:26.590064Z","shell.execute_reply.started":"2023-06-18T16:01:25.685404Z","shell.execute_reply":"2023-06-18T16:01:26.588898Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"                                                       en  \\\n664                                            Editorial.   \n1291                                           Editorial.   \n2163                                          Drugs news.   \n2750                                           Editorial.   \n3727              Misophonia and contemporary psychiatry.   \n...                                                   ...   \n738932  Unfortunately, program decisions for training ...   \n738933  This series of three articles, intended for th...   \n738934  With these 14 recommendations on how to get a ...   \n738935  They are offered to minimize the chances of ac...   \n740643               Systematic review and meta-analysis.   \n\n                                                       fr  \n664                                            Éditorial.  \n1291                                           Éditorial.  \n2163                                    Info-médicaments.  \n2750                                           Éditorial.  \n3727             Misophonie et psychiatrie contemporaine.  \n...                                                   ...  \n738932  Malheureusement, la décision de la direction d...  \n738933  Cette série de trois articles, destinée aux re...  \n738934  La mise en œuvre de ces 14 recommandations sur...  \n738935  Elles sont proposées pour éviter la révision d...  \n740643                Revue systématique et méta-analyse.  \n\n[39540 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>fr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>664</th>\n      <td>Editorial.</td>\n      <td>Éditorial.</td>\n    </tr>\n    <tr>\n      <th>1291</th>\n      <td>Editorial.</td>\n      <td>Éditorial.</td>\n    </tr>\n    <tr>\n      <th>2163</th>\n      <td>Drugs news.</td>\n      <td>Info-médicaments.</td>\n    </tr>\n    <tr>\n      <th>2750</th>\n      <td>Editorial.</td>\n      <td>Éditorial.</td>\n    </tr>\n    <tr>\n      <th>3727</th>\n      <td>Misophonia and contemporary psychiatry.</td>\n      <td>Misophonie et psychiatrie contemporaine.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>738932</th>\n      <td>Unfortunately, program decisions for training ...</td>\n      <td>Malheureusement, la décision de la direction d...</td>\n    </tr>\n    <tr>\n      <th>738933</th>\n      <td>This series of three articles, intended for th...</td>\n      <td>Cette série de trois articles, destinée aux re...</td>\n    </tr>\n    <tr>\n      <th>738934</th>\n      <td>With these 14 recommendations on how to get a ...</td>\n      <td>La mise en œuvre de ces 14 recommandations sur...</td>\n    </tr>\n    <tr>\n      <th>738935</th>\n      <td>They are offered to minimize the chances of ac...</td>\n      <td>Elles sont proposées pour éviter la révision d...</td>\n    </tr>\n    <tr>\n      <th>740643</th>\n      <td>Systematic review and meta-analysis.</td>\n      <td>Revue systématique et méta-analyse.</td>\n    </tr>\n  </tbody>\n</table>\n<p>39540 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"tempDataset_dedup = tempDataset.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2023-06-18T18:27:30.025390Z","iopub.execute_input":"2023-06-18T18:27:30.025762Z","iopub.status.idle":"2023-06-18T18:27:31.062919Z","shell.execute_reply.started":"2023-06-18T18:27:30.025732Z","shell.execute_reply":"2023-06-18T18:27:31.062189Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#Declaring helper functions separately\nimport langid, string\nfrom tqdm import tqdm\ntqdm.pandas() #So I can see progress\ncount = lambda l1,l2: sum([1 for x in l1 if x in l2])","metadata":{"execution":{"iopub.status.busy":"2023-06-18T18:45:48.220559Z","iopub.execute_input":"2023-06-18T18:45:48.220956Z","iopub.status.idle":"2023-06-18T18:45:48.227650Z","shell.execute_reply.started":"2023-06-18T18:45:48.220928Z","shell.execute_reply":"2023-06-18T18:45:48.226279Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"#Having verified that we can convert this back into a Dataset object, we'll just \n#do everything else in a pandas DataFrame. There are five per-sentence checks\n#we can do.\n\ndef retain_sentence(row): #This defines a good sentence we wish to retain \n    for sentence in (row[\"en\"], row[\"fr\"]):\n        #Exclude sentences with mismatched () or '' or \"\"\n        if ((sentence.count(\"(\") != sentence.count(\")\")) or \n            (sentence.count(\"'\") % 2 != 0) or\n            (sentence.count('\"') % 2 != 0)):\n            return False\n        sent_length = len(sentence)\n        #Exclude sentences with punctuation percentage > 0.4\n        if (count(sentence,set(string.punctuation)) > 0.4 * sent_length):\n            return False\n        #Exclude sentences with > 150 words\n        num_words = len(sentence.split(\" \"))\n        if (num_words > 150):\n            return False\n        #Exclude sentences with char-to-word ratio > 12 or < 1.5\n        c2w_ratio = sent_length / num_words\n        if ((c2w_ratio > 12) or (c2w_ratio < 1.5)):\n            return False\n    #Exclude sentences which don't match the correct language (costly)\n    if ((langid.classify(row['en'])[0] != \"en\") or \n        (langid.classify(row['fr'])[0] != \"fr\")):\n        return False\n    #There is one more step which involves checking sentence alignment; we will\n    #do that on the command line because it's a bit more involved.\n    return True","metadata":{"execution":{"iopub.status.busy":"2023-06-18T18:45:53.019656Z","iopub.execute_input":"2023-06-18T18:45:53.020065Z","iopub.status.idle":"2023-06-18T18:45:53.029422Z","shell.execute_reply.started":"2023-06-18T18:45:53.020019Z","shell.execute_reply":"2023-06-18T18:45:53.027730Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"tempDataset_nearlyAligned = tempDataset_dedup.progress_apply(retain_sentence, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-06-18T18:45:57.468001Z","iopub.execute_input":"2023-06-18T18:45:57.468463Z","iopub.status.idle":"2023-06-18T19:12:02.566381Z","shell.execute_reply.started":"2023-06-18T18:45:57.468433Z","shell.execute_reply":"2023-06-18T19:12:02.564735Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"100%|██████████| 701760/701760 [26:05<00:00, 448.38it/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"filtered_set = tempDataset_dedup[tempDataset_nearlyAligned]","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:13:50.433242Z","iopub.execute_input":"2023-06-18T19:13:50.433905Z","iopub.status.idle":"2023-06-18T19:13:50.505389Z","shell.execute_reply.started":"2023-06-18T19:13:50.433868Z","shell.execute_reply":"2023-06-18T19:13:50.504362Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"filtered_set = filtered_set.reset_index()","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:21:50.345782Z","iopub.execute_input":"2023-06-18T19:21:50.346195Z","iopub.status.idle":"2023-06-18T19:21:50.363014Z","shell.execute_reply.started":"2023-06-18T19:21:50.346165Z","shell.execute_reply":"2023-06-18T19:21:50.361594Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"f = open(\"filtered.txt\", \"w\", encoding = \"utf-8\")\nsource = filtered_set[\"en\"]\ntarget = filtered_set[\"fr\"]\nfor i in range(len(filtered_set)):\n    f.write(source[i] + \"\\t\" + target[i] + \"\\n\")\nf.close()","metadata":{"execution":{"iopub.status.busy":"2023-06-18T19:23:57.927937Z","iopub.execute_input":"2023-06-18T19:23:57.928328Z","iopub.status.idle":"2023-06-18T19:24:00.886761Z","shell.execute_reply.started":"2023-06-18T19:23:57.928299Z","shell.execute_reply":"2023-06-18T19:24:00.885321Z"},"trusted":true},"execution_count":48,"outputs":[]}]}