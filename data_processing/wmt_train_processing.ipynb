{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f4c7f77-2fca-4846-9001-5195c038ef77",
   "metadata": {},
   "source": [
    "## WMT Training Data Processing\n",
    "This notebook will be used for devising a script to parse WMT BTT training sets.\n",
    "\n",
    "### Start State\n",
    "We have three WMT BTT training sets - 2016, 2019, and 2022. They are in three slightly different formats, requiring bespoke parsing.\n",
    "- The 2016 data is in a big .txt file about 100MB in size. Each line is formatted as follows: PubMed ID | \\[Source sentence\\].|Target Sentence. A quick inspection of the dataset shows that some source sentences are marked as \"Not Available\" and others are marked as \"In Process Citation\". We must discard these. Quite a few sentences also have double quotation marks replaced with \\&quot; which we must subsitute.\n",
    "- The 2019 data is in a folder comprising aligned parallel abstracts in files of the format PMID_en.txt and PMID_fr.txt. Within each abstract, sentences beginning with # are not translated, and can be ignored.\n",
    "- The 2022 data is in a folder comprising unaligned parallel abstracts in files of the format PMID_en.txt and PMID_fr.txt. Following Choi et al. (2022), we will split these sentences up using a sentence splitter and only consider those abstracts with the same number of sentences as our parallel corpus. \n",
    "\n",
    "### Desired Outcome\n",
    "For every training set, we want to generate a single, **separate** .txt file containing aligned pairs of source and target sentences in the format `source_sentence (TAB) target_sentence (newline)`. This is more convenient for evaluation sets, for which we will use the HuggingFace `Trainer` API. We will concatenate them later. Note that we haven't started preprocessing our data - we will use Wu et al.(2022)'s method for this later.\n",
    "\n",
    "### Considerations\n",
    "- HuggingFace's programmatic Dataset upload API is broken; this issue is still unresolved and cannot be overcome by switching to earlier versions. We therefore have to preprocess and upload via the web interface.\n",
    "- We cannot use a CSV, because the `datasets` library's to_csv function adds strange characters to the data, even with the correct UTF encoding applied. In essence, converting to CSV and uploading _that_ leads to data corruption. Intuitively, we must use a .txt file and upload that via the web interface - end-users will load our dataset as a Dataset or DatasetDict object, and the .txt file source will be transparent to them. I'll write up a script to process these .txt files, since they will all be in the same format.\n",
    "- We won't be using pandas here, as our sentences are not split into multiple tab-separated columns (which pandas is good for). We'll just read the .txt files and process them as we go.\n",
    "\n",
    "### Unpacking\n",
    "We've already unpacked the tar.gz files using `tar -xzf`; I've renamed the WMT19 training data directory to `wmt19_enfr_train` to be more consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a51280-7781-4e26-afd4-4f0133a16e53",
   "metadata": {},
   "source": [
    "Let's begin with the 2016 data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8cc9cc7-b4b5-4934-99df-4977df7614ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get current working directory\n",
    "import os\n",
    "dir_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "743b9124-2f79-44ba-abed-227b6afa31f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS = {16:\"raw_training_data/wmt16_enfr_train.txt\", 19:\"raw_training_data/wmt19_enfr_train\", 22: \"raw_training_data/wmt22_enfr_train\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e69ab1e7-1ba6-42e5-8f8e-4dfd74ea9609",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_2016 = open(os.path.join(dir_path, PATHS[16]),  \"r\", encoding = \"utf8\")\n",
    "output = open(os.path.join(dir_path, \"wmt16_train.txt\"), \"w\", encoding = \"utf8\")\n",
    "#print(repr(wmt_2016.readline().strip())) #No hidden characters\n",
    "\n",
    "#We will preprocess line by line into a fresh .txt file.\n",
    "\n",
    "for line in wmt_2016.readlines():\n",
    "    line = line.strip(); #Remove trailing \\n\n",
    "    content = line.split(\"|\") #Split string into list of PMID, source, and target\n",
    "    source = content[1]\n",
    "    if ((source == \"[Not Available].\") or (\"In Process Citation\" in source) or (\"In process citation\" in source) or (\"In process Citation\" in source)): #No source translation\n",
    "        continue\n",
    "    #Remove delimiters and weird characters\n",
    "    source = source.replace(\"[\", \"\")\n",
    "    source = source.replace(\"]\", \"\")\n",
    "    source = source.replace(\"&quot;\", '\"')\n",
    "    #Full stops are added to source sentences regardless of ending punctuation, but we cannot do a block replacement without affecting !... and ?..., so this is the best we can do.\n",
    "    if ((source[-2:] == \"?.\") or (source[-2:] == \"!.\")): \n",
    "        source = source[:-1]\n",
    "    target = content[2]\n",
    "    if ((\"In process citation\" in target) or (\"In Process Citation\" in target)): #No target translation\n",
    "        continue\n",
    "    target = target.replace(\"&quot;\", '\"')\n",
    "    if ((target[-2:] == \"?.\") or (target[-2:] == \"!.\")):\n",
    "        target = target[:-1]\n",
    "    \n",
    "    output.write(source + \"\\t\" + target + \"\\n\")\n",
    "\n",
    "wmt_2016.close()\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b33276d-c060-4c54-a0ac-b710d331bebf",
   "metadata": {},
   "source": [
    "Looking through the resultant .txt file, it's not too bad - we have some misalignments, but this is to be expected, and we can rectify these later during preprocessing. Right now, we just want to convert our file into an obedient format, and we have done this successfully. Let's move on to the 2019 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f616aebe-1e74-4fcc-970a-de9f1e954592",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_2019 = os.listdir(os.path.join(dir_path, PATHS[19])) #Gives us all the names of the files in the directory; happily, these are all sorted. They're aligned, too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "759c27cc-738c-4bf2-b0fa-998a99a04c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20847962_fr.txt\n"
     ]
    }
   ],
   "source": [
    "print(files_2019[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c18afc7-4a44-4aa6-ab00-512f24285467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first iterate through the list to check whether we have unmatched files (always a possibility).\n",
    "for fileNum in range(0, len(files_2019), 2):\n",
    "    if(files_2019[fileNum][:-7] != files_2019[fileNum + 1][:-7]):\n",
    "        print(\"oops!\") #Wow, there really aren't any unmatched files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68d82211-96a6-4198-a220-418c72d72341",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_2019 = os.path.join(dir_path, PATHS[19]) + \"/\"\n",
    "output = open(os.path.join(dir_path, \"wmt19_train.txt\"), \"w\", encoding = \"utf8\")\n",
    "for i in range(0, len(files_2019), 2):\n",
    "    f1 = open(os.path.join(path_2019, files_2019[i]), \"r\", encoding = \"utf8\")\n",
    "    sourceSentences = f1.readlines()\n",
    "    f2 = open(os.path.join(path_2019, files_2019[i + 1]), \"r\", encoding = \"utf8\")\n",
    "    targetSentences = f2.readlines()\n",
    "    for j in range(len(sourceSentences)):\n",
    "        if(sourceSentences[j][0] == \"#\"): #Ignore untranslated sentences\n",
    "            continue\n",
    "        #We need to remove untranslated article names; these are found in the first sentence after the # marks. \n",
    "        #Generally, we can get around this with not much loss by starting the source sentence from BACKGROUND, which skips the title. \n",
    "        #Subsequent preprocessing will filter out poorly-aligned sentences - this is just an early labour-saving step.\n",
    "        startOfTranslated = sourceSentences[j].find(\"BACKGROUND\")\n",
    "        if(startOfTranslated != -1):\n",
    "            sourceSentences[j] = sourceSentences[j][startOfTranslated:]\n",
    "        if(sourceSentences[j][0] == \"[\"): #Some other sentences have the untranslated article name in front of the translated sentences; the name is enclosed in square brackets and ends with a full stop.\n",
    "            endOfTitle = sourceSentences[j].find(\"]\") + 2 #We want the first char after the full stop\n",
    "            sourceSentences[j] = sourceSentences[j][endOfTitle:]\n",
    "            if (sourceSentences[j].strip() == \"\"):\n",
    "                continue\n",
    "        translate_msg = targetSentences[j].find(\"[Traduction par l’éditeur].\") #A few target sentences have this message appended to them - Translation by the editor. We omit this.\n",
    "        if(translate_msg != -1):\n",
    "            targetSentences[j] = targetSentences[j][:translate_msg]\n",
    "            if (targetSentences[j].strip() == \"\"):\n",
    "                continue\n",
    "        output.write(sourceSentences[j].strip() + \"\\t\" + targetSentences[j].strip() + \"\\n\") #Get rid of whitespace\n",
    "    f1.close()\n",
    "    f2.close()\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f56eec-58c3-432a-9ee3-918097248632",
   "metadata": {},
   "source": [
    "It seems okay! Let's move on to the 2022 data now. We found an unpaired file - 32479674_en.txt - so we removed that. We also found a non EN/FR file, 32514214_no.txt; we also removed that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "217e35b3-fdc1-4d9b-a5e4-371dea793829",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_2022 = os.listdir(os.path.join(dir_path, PATHS[22])) #Gives us all the names of the files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2afb4070-20d3-4df4-a77e-78555d49907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first iterate through the list to check whether we have unmatched files (always a possibility).\n",
    "for fileNum in range(0, len(files_2022), 2):\n",
    "    if(files_2022[fileNum][:-7] != files_2022[fileNum + 1][:-7]):\n",
    "        print(files_2022[fileNum]) #All is well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd1c5f40-ccff-4f2c-851f-57f86c60c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following Choi et al.(2022), we will use MosesSentenceSplitter to split our many-sentence-in-one-line abstracts.\n",
    "#from mosestokenizer import MosesSentenceSplitter #Very, very slow, likely due to its unwrapping ability. We won't need that here - we will use a sentence aligner to check later on.\n",
    "from sentence_splitter import SentenceSplitter #Source: https://libraries.io/pypi/sentence-splitter\n",
    "sourceSplitter = SentenceSplitter(language='en')\n",
    "targetSplitter = SentenceSplitter(language='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "363c8d61-5470-4281-86f6-03d603d1958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_2022 = os.path.join(dir_path, PATHS[22]) + \"/\"\n",
    "output = open(os.path.join(dir_path, \"wmt22_train.txt\"), \"w\", encoding = \"utf8\")\n",
    "for i in range(0, len(files_2022), 2):\n",
    "    f1 = open(os.path.join(path_2022, files_2022[i]), \"r\", encoding = \"utf8\")\n",
    "    sourceSentences = sourceSplitter.split(text=f1.readline())\n",
    "    f2 = open(os.path.join(path_2022, files_2022[i + 1]), \"r\", encoding = \"utf8\")\n",
    "    targetSentences = targetSplitter.split(text=f2.readline())\n",
    "    if (len(sourceSentences) != len(targetSentences)): #If there are more sentences in either, we don't know which should be aligned with which, so ignore (per Choi et al.)\n",
    "        continue\n",
    "    for j in range(len(sourceSentences)):\n",
    "        output.write(sourceSentences[j].strip() + \"\\t\" + targetSentences[j].strip() + \"\\n\") #Get rid of whitespace\n",
    "    f1.close()\n",
    "    f2.close()\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19f6e1c-c13f-45fc-80e8-9db8274117b9",
   "metadata": {},
   "source": [
    "This was adequate for a first pass, but we note that there are several HTML tags, such as \\<i>,\\<sub>, \\<sup>, and so on. Yet, this is fine - these appear in the test set, too! I think we can leave them in, because we need to keep the test set the same as the WMT22 BTT anyway. Now, it's time to concatenate all the training data. This doesn't impose extra work for preprocessing, because the first step in preprocessing is to remove duplicates, and those must be captured across all our training data. Subsequently, when we add more data to our training corpus using various means, we'll repeat this preprocessing step over the new training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a298bd00-33ca-4c39-880a-8f08d50c21b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_output = open(os.path.join(dir_path, \"wmt_parallel_train.txt\"), \"w\", encoding = \"utf8\")\n",
    "train_2016 = open(os.path.join(dir_path, \"wmt16_train.txt\"), \"r\", encoding = \"utf8\")\n",
    "for line in train_2016.readlines():\n",
    "    final_train_output.write(line)\n",
    "train_2019 = open(os.path.join(dir_path, \"wmt19_train.txt\"), \"r\", encoding = \"utf8\")\n",
    "for line in train_2019.readlines():\n",
    "    final_train_output.write(line)\n",
    "train_2022 = open(os.path.join(dir_path, \"wmt22_train.txt\"), \"r\", encoding = \"utf8\")\n",
    "for line in train_2022.readlines():\n",
    "    final_train_output.write(line)\n",
    "final_train_output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb6d204-45c0-4e98-b92c-8d9e5714244e",
   "metadata": {},
   "source": [
    "And we've generated our initial in-domain parallel corpus! Our next step will be preprocessing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
