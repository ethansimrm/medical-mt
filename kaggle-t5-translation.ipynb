{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-25T18:55:45.756014Z","iopub.execute_input":"2023-05-25T18:55:45.756510Z","iopub.status.idle":"2023-05-25T18:55:45.769337Z","shell.execute_reply.started":"2023-05-25T18:55:45.756474Z","shell.execute_reply":"2023-05-25T18:55:45.768074Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install transformers==4.28.0 datasets evaluate sacrebleu torch git+https://github.com/huggingface/accelerate","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:55:45.771185Z","iopub.execute_input":"2023-05-25T18:55:45.771547Z","iopub.status.idle":"2023-05-25T18:55:49.862947Z","shell.execute_reply.started":"2023-05-25T18:55:45.771512Z","shell.execute_reply":"2023-05-25T18:55:49.861829Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/huggingface/accelerate\n  Cloning https://github.com/huggingface/accelerate to /tmp/pip-req-build-bntycgy2\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate /tmp/pip-req-build-bntycgy2\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(\"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\")","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:55:49.865379Z","iopub.execute_input":"2023-05-25T18:55:49.868955Z","iopub.status.idle":"2023-05-25T18:55:50.293533Z","shell.execute_reply.started":"2023-05-25T18:55:49.868913Z","shell.execute_reply":"2023-05-25T18:55:50.290257Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"#Load in EN-FR subset of OPUS books\nfrom datasets import load_dataset, load_dataset_builder\nbooks = load_dataset(\"opus_books\", \"en-fr\")","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:55:50.295337Z","iopub.execute_input":"2023-05-25T18:55:50.295667Z","iopub.status.idle":"2023-05-25T18:56:02.600708Z","shell.execute_reply.started":"2023-05-25T18:55:50.295635Z","shell.execute_reply":"2023-05-25T18:56:02.599728Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0132987fb4040f1b55e66236f23eddf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/7.98k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cbc7d72cca34987b38ffebd8dd24fb1"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset opus_books/en-fr (download: 11.45 MiB, generated: 31.47 MiB, post-processed: Unknown size, total: 42.92 MiB) to /root/.cache/huggingface/datasets/opus_books/en-fr/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/12.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"677977650a004de0a8113974eba2224f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/127085 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset opus_books downloaded and prepared to /root/.cache/huggingface/datasets/opus_books/en-fr/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b52ba1a951ac4a4783d9934ca45210e2"}},"metadata":{}}]},{"cell_type":"code","source":"#ds_builder = load_dataset_builder(\"rotten_tomatoes\")\n#ds_builder = load_dataset_builder(\"wmt14\", 'fr-en')\n#ds_builder.info.description\n#ds_builder.info.features","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:56:02.603512Z","iopub.execute_input":"2023-05-25T18:56:02.604101Z","iopub.status.idle":"2023-05-25T18:56:02.608385Z","shell.execute_reply.started":"2023-05-25T18:56:02.604073Z","shell.execute_reply":"2023-05-25T18:56:02.607127Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Entirety of OPUS books is a training dataset, so split into train:test with 80:20.\nbooks = books[\"train\"].train_test_split(test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:56:02.609589Z","iopub.execute_input":"2023-05-25T18:56:02.611248Z","iopub.status.idle":"2023-05-25T18:56:02.677400Z","shell.execute_reply.started":"2023-05-25T18:56:02.611211Z","shell.execute_reply":"2023-05-25T18:56:02.676570Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Inspect data; the split is random.\nbooks[\"train\"][0]","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:56:02.678983Z","iopub.execute_input":"2023-05-25T18:56:02.679412Z","iopub.status.idle":"2023-05-25T18:56:02.687203Z","shell.execute_reply.started":"2023-05-25T18:56:02.679341Z","shell.execute_reply":"2023-05-25T18:56:02.686264Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'id': '58498',\n 'translation': {'en': 'From time to time, the old lady addressed him in a very low tone, and he replied as well as he was able, with a sort of awkward and constrained politeness.',\n  'fr': 'De temps en temps la vieille dame lui adressait la parole tout bas, et il lui répondait de son mieux avec une sorte de politesse gauche et contrainte.'}}"},"metadata":{}}]},{"cell_type":"code","source":"#Tokenise data - we must load the correct tokeniser for our model before we input parameters. This is based on SentencePiece.\nfrom transformers import AutoTokenizer\ncheckpoint = \"t5-small\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n#NB: As long as data looks like {\"input\" : \"XXXX\" , \"target\" : \"YYYY\"}, it can be processed.","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:56:02.688562Z","iopub.execute_input":"2023-05-25T18:56:02.689109Z","iopub.status.idle":"2023-05-25T18:56:06.667570Z","shell.execute_reply.started":"2023-05-25T18:56:02.689077Z","shell.execute_reply":"2023-05-25T18:56:06.666627Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d23d62f5761540e6a66cd4c83315d364"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a33bd93d5f8c4a16b7733a2aed331ddd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"194a498d980a4002ba83cfa227e22ffe"}},"metadata":{}}]},{"cell_type":"code","source":"#Specify a preprocessing function which allows us to tokenise source and target languages correctly AND prime T5 with the correct prompt before each sentence\nsource_lang = \"en\"\ntarget_lang = \"fr\"\nprefix = \"translate English to French: \"\n\n\ndef preprocess_function(examples):\n    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]] \n    #We are essentially querying the dictionary here, whereby books[\"train\"][\"translation\"][0][\"en\"] references the first English sentence above.\n    targets = [example[target_lang] for example in examples[\"translation\"]]\n    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True) #We will truncate sentences longer than 128 words long\n    return model_inputs\n\n#books[\"train\"][\"translation\"][0][\"en\"] will give us the first English sentence","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:56:06.669289Z","iopub.execute_input":"2023-05-25T18:56:06.670002Z","iopub.status.idle":"2023-05-25T18:56:06.676114Z","shell.execute_reply.started":"2023-05-25T18:56:06.669964Z","shell.execute_reply":"2023-05-25T18:56:06.675249Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Apply this function over the training dataset over multiple elements simultaneously using the batched = True argument.\ntokenized_books = books.map(preprocess_function, batched=True) ","metadata":{"execution":{"iopub.status.busy":"2023-05-25T18:56:06.677688Z","iopub.execute_input":"2023-05-25T18:56:06.678353Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/102 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed493c056a574f9da12218bceb6ec1e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/26 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed7b3090d2af4a63a550834bfc6e1a8c"}},"metadata":{}}]},{"cell_type":"code","source":"#Create a batch of examples and dynamically pad to hit length of longest sentence per batch\nfrom transformers import DataCollatorForSeq2Seq\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load evaluation method\nimport evaluate\nmetric = evaluate.load(\"sacrebleu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef postprocess_text(preds, labels): \n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    return preds, labels\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True) #Convert back into words\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id) #Ignore padded labels added by the data collator to the test set\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels) #Remove leading and trailing spaces\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels) #BLEU score for provided input and references\n    result = {\"bleu\": result[\"score\"]}\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens) #Compute mean prediction length\n    result = {k: round(v, 4) for k, v in result.items()} #Round score to 4dp\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ready to download model\nfrom transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint) #Model is 242MB in size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments( #Collects hyperparameters\n    output_dir=\"test_t5_small_example_kaggle\",\n    evaluation_strategy=\"epoch\", #Evaluates at the end of each epoch\n    learning_rate=2e-5, #Initial learning rate for AdamW\n    per_device_train_batch_size=16, #Minibatch learning\n    per_device_eval_batch_size=16, #Batch size for evaluation\n    weight_decay=0.01, #Weight decay for loss computation; Loss = Loss + WD * sum (weights squared)\n    save_total_limit=3, #Number of checkpoints to save\n    num_train_epochs=2,\n    predict_with_generate=True, #Use with ROUGE/BLEU and other translation metrics (see below)\n    fp16=True, #Remove fp16 = True if not using CUDA\n    push_to_hub=True,\n)\n\ntrainer = Seq2SeqTrainer( #Saves us from writing our own training loops\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_books[\"train\"],\n    eval_dataset=tokenized_books[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n#However, these metrics require that we generate some text with the model rather than a single forward pass as with e.g. classification. \n#The Seq2SeqTrainer allows for the use of the generate method when setting predict_with_generate=True which will generate text for each sample in the evaluation set. \n#That means we evaluate generated text within the compute_metric function. We just need to decode the predictions and labels first.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"translate English to French: Legumes share resources with nitrogen-fixing bacteria.\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\ntranslator = pipeline(\"translation\", model=\"test_t5_small_example_2\")\ntranslator(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}